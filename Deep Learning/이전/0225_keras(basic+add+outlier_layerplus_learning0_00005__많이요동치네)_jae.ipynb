{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "0225_keras(basic+add+outlier_layerplus_learning0.00005__많이요동치네)_jae.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starlidust/Astro/blob/master/0225_keras(basic%2Badd%2Boutlier_layerplus_learning0_00005__%EB%A7%8E%EC%9D%B4%EC%9A%94%EB%8F%99%EC%B9%98%EB%84%A4)_jae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAnG9WAJIdG3",
        "colab_type": "code",
        "outputId": "4c3a74ac-7959-4eef-e2ff-3933278fe983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "import tensorflow as tf\n",
        "import chart_studio.plotly as py\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow import keras"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ratlOsTrIqkA",
        "colab_type": "code",
        "outputId": "9fc37f16-0a66-4ea0-98a8-38a62f0d6966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjaMyZkkI36o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/data_con/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TJVuvADIdHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"./data/\" \n",
        "train_df = pd.read_csv(path+'train.csv',index_col=0)\n",
        "test_df = pd.read_csv(path+'test.csv',index_col=0)\n",
        "sample_submission_df = pd.read_csv(path+'sample_submission.csv',index_col=0)\n",
        "pd.options.display.max_columns = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhfz2BXRIdHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##conda install keras-gpu 하면 gpu도 괴롭힐 수 있음"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZsLrP8DIdHL",
        "colab_type": "text"
      },
      "source": [
        "## 전처리\n",
        " - 이상치 확인 및 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdNREsVMIdHN",
        "colab_type": "code",
        "outputId": "85f796ce-2aec-40a3-ac96-ff4cc186b7d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(199991, 22)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6PAAhPuIdHS",
        "colab_type": "code",
        "outputId": "fe2ed7d2-b4f7-4852-dd0a-2d4c7d879611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "train_df.describe()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fiberID</th>\n",
              "      <th>psfMag_u</th>\n",
              "      <th>psfMag_g</th>\n",
              "      <th>psfMag_r</th>\n",
              "      <th>psfMag_i</th>\n",
              "      <th>psfMag_z</th>\n",
              "      <th>fiberMag_u</th>\n",
              "      <th>fiberMag_g</th>\n",
              "      <th>fiberMag_r</th>\n",
              "      <th>fiberMag_i</th>\n",
              "      <th>fiberMag_z</th>\n",
              "      <th>petroMag_u</th>\n",
              "      <th>petroMag_g</th>\n",
              "      <th>petroMag_r</th>\n",
              "      <th>petroMag_i</th>\n",
              "      <th>petroMag_z</th>\n",
              "      <th>modelMag_u</th>\n",
              "      <th>modelMag_g</th>\n",
              "      <th>modelMag_r</th>\n",
              "      <th>modelMag_i</th>\n",
              "      <th>modelMag_z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>199991.000000</td>\n",
              "      <td>1.999910e+05</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>1.999910e+05</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "      <td>199991.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>360.830152</td>\n",
              "      <td>-6.750146e+00</td>\n",
              "      <td>18.675373</td>\n",
              "      <td>18.401235</td>\n",
              "      <td>18.043495</td>\n",
              "      <td>17.663526</td>\n",
              "      <td>1.084986e+01</td>\n",
              "      <td>19.072693</td>\n",
              "      <td>19.134483</td>\n",
              "      <td>18.183331</td>\n",
              "      <td>18.000882</td>\n",
              "      <td>21.837903</td>\n",
              "      <td>18.454136</td>\n",
              "      <td>18.481525</td>\n",
              "      <td>17.686617</td>\n",
              "      <td>17.699207</td>\n",
              "      <td>20.110991</td>\n",
              "      <td>18.544375</td>\n",
              "      <td>18.181544</td>\n",
              "      <td>17.692395</td>\n",
              "      <td>17.189281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>225.305890</td>\n",
              "      <td>1.187678e+04</td>\n",
              "      <td>155.423024</td>\n",
              "      <td>127.128078</td>\n",
              "      <td>116.622194</td>\n",
              "      <td>123.735298</td>\n",
              "      <td>4.172116e+03</td>\n",
              "      <td>749.256162</td>\n",
              "      <td>90.049058</td>\n",
              "      <td>122.378972</td>\n",
              "      <td>145.862346</td>\n",
              "      <td>789.472333</td>\n",
              "      <td>154.376277</td>\n",
              "      <td>97.240448</td>\n",
              "      <td>145.730872</td>\n",
              "      <td>142.691880</td>\n",
              "      <td>122.299062</td>\n",
              "      <td>161.728183</td>\n",
              "      <td>133.984475</td>\n",
              "      <td>131.183416</td>\n",
              "      <td>133.685138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-5.310802e+06</td>\n",
              "      <td>-40022.466071</td>\n",
              "      <td>-27184.795793</td>\n",
              "      <td>-26566.310827</td>\n",
              "      <td>-24878.828280</td>\n",
              "      <td>-1.864766e+06</td>\n",
              "      <td>-215882.917191</td>\n",
              "      <td>-21802.656144</td>\n",
              "      <td>-20208.516262</td>\n",
              "      <td>-26505.602101</td>\n",
              "      <td>-24463.431833</td>\n",
              "      <td>-25958.752324</td>\n",
              "      <td>-23948.588523</td>\n",
              "      <td>-40438.184078</td>\n",
              "      <td>-30070.729379</td>\n",
              "      <td>-26236.578659</td>\n",
              "      <td>-36902.402336</td>\n",
              "      <td>-36439.638493</td>\n",
              "      <td>-38969.416822</td>\n",
              "      <td>-26050.710196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>174.000000</td>\n",
              "      <td>1.965259e+01</td>\n",
              "      <td>18.701180</td>\n",
              "      <td>18.048572</td>\n",
              "      <td>17.747663</td>\n",
              "      <td>17.425523</td>\n",
              "      <td>1.994040e+01</td>\n",
              "      <td>18.902851</td>\n",
              "      <td>18.259352</td>\n",
              "      <td>17.903615</td>\n",
              "      <td>17.606148</td>\n",
              "      <td>19.247795</td>\n",
              "      <td>18.113933</td>\n",
              "      <td>17.479794</td>\n",
              "      <td>17.050294</td>\n",
              "      <td>16.804705</td>\n",
              "      <td>19.266214</td>\n",
              "      <td>18.076120</td>\n",
              "      <td>17.423425</td>\n",
              "      <td>16.977671</td>\n",
              "      <td>16.705774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>349.000000</td>\n",
              "      <td>2.087136e+01</td>\n",
              "      <td>19.904235</td>\n",
              "      <td>19.454492</td>\n",
              "      <td>19.043895</td>\n",
              "      <td>18.611799</td>\n",
              "      <td>2.104910e+01</td>\n",
              "      <td>20.069038</td>\n",
              "      <td>19.631419</td>\n",
              "      <td>19.188763</td>\n",
              "      <td>18.710967</td>\n",
              "      <td>20.366848</td>\n",
              "      <td>19.586559</td>\n",
              "      <td>19.182789</td>\n",
              "      <td>18.693370</td>\n",
              "      <td>18.174592</td>\n",
              "      <td>20.406840</td>\n",
              "      <td>19.547674</td>\n",
              "      <td>19.143156</td>\n",
              "      <td>18.641756</td>\n",
              "      <td>18.100997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>526.000000</td>\n",
              "      <td>2.216043e+01</td>\n",
              "      <td>21.150297</td>\n",
              "      <td>20.515936</td>\n",
              "      <td>20.073528</td>\n",
              "      <td>19.883760</td>\n",
              "      <td>2.233754e+01</td>\n",
              "      <td>21.385830</td>\n",
              "      <td>20.773911</td>\n",
              "      <td>20.331419</td>\n",
              "      <td>20.133179</td>\n",
              "      <td>21.797480</td>\n",
              "      <td>21.004397</td>\n",
              "      <td>20.457491</td>\n",
              "      <td>20.019112</td>\n",
              "      <td>19.807652</td>\n",
              "      <td>21.992898</td>\n",
              "      <td>20.962386</td>\n",
              "      <td>20.408140</td>\n",
              "      <td>19.968846</td>\n",
              "      <td>19.819554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1.877392e+04</td>\n",
              "      <td>3538.984910</td>\n",
              "      <td>3048.110913</td>\n",
              "      <td>4835.218639</td>\n",
              "      <td>9823.740407</td>\n",
              "      <td>4.870154e+03</td>\n",
              "      <td>248077.513380</td>\n",
              "      <td>12084.735440</td>\n",
              "      <td>8059.638535</td>\n",
              "      <td>18358.921741</td>\n",
              "      <td>298771.019041</td>\n",
              "      <td>12139.815877</td>\n",
              "      <td>7003.136546</td>\n",
              "      <td>9772.190537</td>\n",
              "      <td>17403.789263</td>\n",
              "      <td>14488.251976</td>\n",
              "      <td>10582.058590</td>\n",
              "      <td>12237.951703</td>\n",
              "      <td>4062.499371</td>\n",
              "      <td>7420.534172</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             fiberID      psfMag_u       psfMag_g       psfMag_r  \\\n",
              "count  199991.000000  1.999910e+05  199991.000000  199991.000000   \n",
              "mean      360.830152 -6.750146e+00      18.675373      18.401235   \n",
              "std       225.305890  1.187678e+04     155.423024     127.128078   \n",
              "min         1.000000 -5.310802e+06  -40022.466071  -27184.795793   \n",
              "25%       174.000000  1.965259e+01      18.701180      18.048572   \n",
              "50%       349.000000  2.087136e+01      19.904235      19.454492   \n",
              "75%       526.000000  2.216043e+01      21.150297      20.515936   \n",
              "max      1000.000000  1.877392e+04    3538.984910    3048.110913   \n",
              "\n",
              "            psfMag_i       psfMag_z    fiberMag_u     fiberMag_g  \\\n",
              "count  199991.000000  199991.000000  1.999910e+05  199991.000000   \n",
              "mean       18.043495      17.663526  1.084986e+01      19.072693   \n",
              "std       116.622194     123.735298  4.172116e+03     749.256162   \n",
              "min    -26566.310827  -24878.828280 -1.864766e+06 -215882.917191   \n",
              "25%        17.747663      17.425523  1.994040e+01      18.902851   \n",
              "50%        19.043895      18.611799  2.104910e+01      20.069038   \n",
              "75%        20.073528      19.883760  2.233754e+01      21.385830   \n",
              "max      4835.218639    9823.740407  4.870154e+03  248077.513380   \n",
              "\n",
              "          fiberMag_r     fiberMag_i     fiberMag_z     petroMag_u  \\\n",
              "count  199991.000000  199991.000000  199991.000000  199991.000000   \n",
              "mean       19.134483      18.183331      18.000882      21.837903   \n",
              "std        90.049058     122.378972     145.862346     789.472333   \n",
              "min    -21802.656144  -20208.516262  -26505.602101  -24463.431833   \n",
              "25%        18.259352      17.903615      17.606148      19.247795   \n",
              "50%        19.631419      19.188763      18.710967      20.366848   \n",
              "75%        20.773911      20.331419      20.133179      21.797480   \n",
              "max     12084.735440    8059.638535   18358.921741  298771.019041   \n",
              "\n",
              "          petroMag_g     petroMag_r     petroMag_i     petroMag_z  \\\n",
              "count  199991.000000  199991.000000  199991.000000  199991.000000   \n",
              "mean       18.454136      18.481525      17.686617      17.699207   \n",
              "std       154.376277      97.240448     145.730872     142.691880   \n",
              "min    -25958.752324  -23948.588523  -40438.184078  -30070.729379   \n",
              "25%        18.113933      17.479794      17.050294      16.804705   \n",
              "50%        19.586559      19.182789      18.693370      18.174592   \n",
              "75%        21.004397      20.457491      20.019112      19.807652   \n",
              "max     12139.815877    7003.136546    9772.190537   17403.789263   \n",
              "\n",
              "          modelMag_u     modelMag_g     modelMag_r     modelMag_i  \\\n",
              "count  199991.000000  199991.000000  199991.000000  199991.000000   \n",
              "mean       20.110991      18.544375      18.181544      17.692395   \n",
              "std       122.299062     161.728183     133.984475     131.183416   \n",
              "min    -26236.578659  -36902.402336  -36439.638493  -38969.416822   \n",
              "25%        19.266214      18.076120      17.423425      16.977671   \n",
              "50%        20.406840      19.547674      19.143156      18.641756   \n",
              "75%        21.992898      20.962386      20.408140      19.968846   \n",
              "max     14488.251976   10582.058590   12237.951703    4062.499371   \n",
              "\n",
              "          modelMag_z  \n",
              "count  199991.000000  \n",
              "mean       17.189281  \n",
              "std       133.685138  \n",
              "min    -26050.710196  \n",
              "25%        16.705774  \n",
              "50%        18.100997  \n",
              "75%        19.819554  \n",
              "max      7420.534172  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQyieXjqIdHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def del_outlier(data, min=0, max=60):\n",
        "    up_idx_t=()\n",
        "    dw_idx_t=()\n",
        "    train_light = data.iloc[:,2:]\n",
        "    for i in range(len(train_light.columns)):\n",
        "        col = train_light.columns[i]\n",
        "        up_idx_t+=tuple(data[data[col]>max].index)\n",
        "        dw_idx_t+=tuple(data[data[col]<min].index)\n",
        "    del_idx = set(up_idx_t+dw_idx_t)\n",
        "    \n",
        "    return data[~data.index.isin(del_idx)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOW3yLvUIdHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = del_outlier(train_df, min=-20, max=60)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h63rnOB9IdHg",
        "colab_type": "code",
        "outputId": "5d72ba49-b6f5-419c-a48d-00ef57569434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "train_df.columns, train_df.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Index(['type', 'fiberID', 'psfMag_u', 'psfMag_g', 'psfMag_r', 'psfMag_i',\n",
              "        'psfMag_z', 'fiberMag_u', 'fiberMag_g', 'fiberMag_r', 'fiberMag_i',\n",
              "        'fiberMag_z', 'petroMag_u', 'petroMag_g', 'petroMag_r', 'petroMag_i',\n",
              "        'petroMag_z', 'modelMag_u', 'modelMag_g', 'modelMag_r', 'modelMag_i',\n",
              "        'modelMag_z'],\n",
              "       dtype='object'), (199770, 22))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEriLvN2IdHj",
        "colab_type": "text"
      },
      "source": [
        "## DATA Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5-DMEgwIdHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_number = {}\n",
        "number_columns = {}\n",
        "for i, column in enumerate(sample_submission_df.columns):\n",
        "    column_number[column] = i\n",
        "    number_columns[i] = column\n",
        "    \n",
        "    \n",
        "def to_number(x, dic):\n",
        "    return dic[x]\n",
        "\n",
        "train_df['type_num'] = train_df['type'].apply(lambda x: to_number(x, column_number))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsjNmwYlIdHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_minus_feature(data,test = False):\n",
        "    from itertools import combinations\n",
        "    n = 0\n",
        "    for count in range(5,21,5):\n",
        "        s = 2\n",
        "        if test == True :\n",
        "            s = 1\n",
        "        selected = data.columns[s:].values[n:count]\n",
        "        mag = str.split(selected[0],'_')[0]\n",
        "        for combi in list(combinations(selected,2)):\n",
        "            name_1st = str.split(combi[0],'_')[1]\n",
        "            name_2nd = str.split(combi[1],'_')[1]\n",
        "            data[mag+\"_\"+name_1st+\"-\"+name_2nd] = data[combi[0]]-data[combi[1]]\n",
        "        n=count\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQK90dmoIdHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = add_minus_feature(train_df)\n",
        "test_df = add_minus_feature(test_df,True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNh3Nq8tIdH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = train_df.drop(columns=['type', 'type_num'], axis=1)\n",
        "train_y = train_df['type_num']\n",
        "test_X = test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebv7h9icIdH6",
        "colab_type": "code",
        "outputId": "eadfeb09-c8ee-4e83-a59c-7be2312f5712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "train_X.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fiberID</th>\n",
              "      <th>psfMag_u</th>\n",
              "      <th>psfMag_g</th>\n",
              "      <th>psfMag_r</th>\n",
              "      <th>psfMag_i</th>\n",
              "      <th>psfMag_z</th>\n",
              "      <th>fiberMag_u</th>\n",
              "      <th>fiberMag_g</th>\n",
              "      <th>fiberMag_r</th>\n",
              "      <th>fiberMag_i</th>\n",
              "      <th>fiberMag_z</th>\n",
              "      <th>petroMag_u</th>\n",
              "      <th>petroMag_g</th>\n",
              "      <th>petroMag_r</th>\n",
              "      <th>petroMag_i</th>\n",
              "      <th>...</th>\n",
              "      <th>petroMag_g-i</th>\n",
              "      <th>petroMag_g-z</th>\n",
              "      <th>petroMag_r-i</th>\n",
              "      <th>petroMag_r-z</th>\n",
              "      <th>petroMag_i-z</th>\n",
              "      <th>modelMag_u-g</th>\n",
              "      <th>modelMag_u-r</th>\n",
              "      <th>modelMag_u-i</th>\n",
              "      <th>modelMag_u-z</th>\n",
              "      <th>modelMag_g-r</th>\n",
              "      <th>modelMag_g-i</th>\n",
              "      <th>modelMag_g-z</th>\n",
              "      <th>modelMag_r-i</th>\n",
              "      <th>modelMag_r-z</th>\n",
              "      <th>modelMag_i-z</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>601</td>\n",
              "      <td>23.198224</td>\n",
              "      <td>21.431953</td>\n",
              "      <td>21.314148</td>\n",
              "      <td>21.176553</td>\n",
              "      <td>21.171444</td>\n",
              "      <td>22.581309</td>\n",
              "      <td>21.644453</td>\n",
              "      <td>21.657571</td>\n",
              "      <td>21.387653</td>\n",
              "      <td>21.572827</td>\n",
              "      <td>22.504317</td>\n",
              "      <td>21.431636</td>\n",
              "      <td>21.478312</td>\n",
              "      <td>21.145409</td>\n",
              "      <td>...</td>\n",
              "      <td>0.286226</td>\n",
              "      <td>1.009190</td>\n",
              "      <td>0.332902</td>\n",
              "      <td>1.055866</td>\n",
              "      <td>0.722964</td>\n",
              "      <td>1.283708</td>\n",
              "      <td>1.385054</td>\n",
              "      <td>1.728637</td>\n",
              "      <td>1.601901</td>\n",
              "      <td>0.101347</td>\n",
              "      <td>0.444929</td>\n",
              "      <td>0.318194</td>\n",
              "      <td>0.343582</td>\n",
              "      <td>0.216847</td>\n",
              "      <td>-0.126735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>788</td>\n",
              "      <td>21.431355</td>\n",
              "      <td>20.708104</td>\n",
              "      <td>20.678850</td>\n",
              "      <td>20.703420</td>\n",
              "      <td>20.473229</td>\n",
              "      <td>21.868797</td>\n",
              "      <td>21.029773</td>\n",
              "      <td>20.967054</td>\n",
              "      <td>20.937731</td>\n",
              "      <td>21.063646</td>\n",
              "      <td>21.360701</td>\n",
              "      <td>20.778968</td>\n",
              "      <td>20.889705</td>\n",
              "      <td>20.639812</td>\n",
              "      <td>...</td>\n",
              "      <td>0.139156</td>\n",
              "      <td>0.132308</td>\n",
              "      <td>0.249893</td>\n",
              "      <td>0.243045</td>\n",
              "      <td>-0.006847</td>\n",
              "      <td>0.734428</td>\n",
              "      <td>0.739030</td>\n",
              "      <td>0.799566</td>\n",
              "      <td>0.980641</td>\n",
              "      <td>0.004602</td>\n",
              "      <td>0.065138</td>\n",
              "      <td>0.246213</td>\n",
              "      <td>0.060537</td>\n",
              "      <td>0.241611</td>\n",
              "      <td>0.181074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>427</td>\n",
              "      <td>17.851451</td>\n",
              "      <td>16.727898</td>\n",
              "      <td>16.679677</td>\n",
              "      <td>16.694640</td>\n",
              "      <td>16.641788</td>\n",
              "      <td>18.171890</td>\n",
              "      <td>17.033098</td>\n",
              "      <td>16.999682</td>\n",
              "      <td>17.095999</td>\n",
              "      <td>17.076449</td>\n",
              "      <td>17.867253</td>\n",
              "      <td>16.738784</td>\n",
              "      <td>16.688874</td>\n",
              "      <td>16.744210</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.005426</td>\n",
              "      <td>-0.069222</td>\n",
              "      <td>-0.055336</td>\n",
              "      <td>-0.119132</td>\n",
              "      <td>-0.063796</td>\n",
              "      <td>1.120628</td>\n",
              "      <td>1.176814</td>\n",
              "      <td>1.157885</td>\n",
              "      <td>1.129134</td>\n",
              "      <td>0.056186</td>\n",
              "      <td>0.037257</td>\n",
              "      <td>0.008506</td>\n",
              "      <td>-0.018929</td>\n",
              "      <td>-0.047680</td>\n",
              "      <td>-0.028751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>864</td>\n",
              "      <td>20.789900</td>\n",
              "      <td>20.040371</td>\n",
              "      <td>19.926909</td>\n",
              "      <td>19.843840</td>\n",
              "      <td>19.463270</td>\n",
              "      <td>21.039030</td>\n",
              "      <td>20.317165</td>\n",
              "      <td>20.217898</td>\n",
              "      <td>20.073852</td>\n",
              "      <td>19.794505</td>\n",
              "      <td>20.433907</td>\n",
              "      <td>19.993727</td>\n",
              "      <td>19.985531</td>\n",
              "      <td>19.750917</td>\n",
              "      <td>...</td>\n",
              "      <td>0.242810</td>\n",
              "      <td>0.538610</td>\n",
              "      <td>0.234614</td>\n",
              "      <td>0.530413</td>\n",
              "      <td>0.295800</td>\n",
              "      <td>0.769012</td>\n",
              "      <td>0.880913</td>\n",
              "      <td>1.012598</td>\n",
              "      <td>1.217856</td>\n",
              "      <td>0.111901</td>\n",
              "      <td>0.243586</td>\n",
              "      <td>0.448844</td>\n",
              "      <td>0.131685</td>\n",
              "      <td>0.336943</td>\n",
              "      <td>0.205258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>612</td>\n",
              "      <td>26.454969</td>\n",
              "      <td>23.058767</td>\n",
              "      <td>21.471406</td>\n",
              "      <td>19.504961</td>\n",
              "      <td>18.389096</td>\n",
              "      <td>25.700632</td>\n",
              "      <td>23.629122</td>\n",
              "      <td>21.742750</td>\n",
              "      <td>19.861718</td>\n",
              "      <td>18.810375</td>\n",
              "      <td>25.859229</td>\n",
              "      <td>22.426929</td>\n",
              "      <td>21.673551</td>\n",
              "      <td>19.610012</td>\n",
              "      <td>...</td>\n",
              "      <td>2.816917</td>\n",
              "      <td>4.050788</td>\n",
              "      <td>2.063539</td>\n",
              "      <td>3.297411</td>\n",
              "      <td>1.233871</td>\n",
              "      <td>1.729059</td>\n",
              "      <td>3.401710</td>\n",
              "      <td>5.389723</td>\n",
              "      <td>6.501398</td>\n",
              "      <td>1.672651</td>\n",
              "      <td>3.660663</td>\n",
              "      <td>4.772338</td>\n",
              "      <td>1.988012</td>\n",
              "      <td>3.099688</td>\n",
              "      <td>1.111675</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 61 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    fiberID   psfMag_u   psfMag_g   psfMag_r   psfMag_i   psfMag_z  \\\n",
              "id                                                                   \n",
              "0       601  23.198224  21.431953  21.314148  21.176553  21.171444   \n",
              "1       788  21.431355  20.708104  20.678850  20.703420  20.473229   \n",
              "2       427  17.851451  16.727898  16.679677  16.694640  16.641788   \n",
              "3       864  20.789900  20.040371  19.926909  19.843840  19.463270   \n",
              "4       612  26.454969  23.058767  21.471406  19.504961  18.389096   \n",
              "\n",
              "    fiberMag_u  fiberMag_g  fiberMag_r  fiberMag_i  fiberMag_z  petroMag_u  \\\n",
              "id                                                                           \n",
              "0    22.581309   21.644453   21.657571   21.387653   21.572827   22.504317   \n",
              "1    21.868797   21.029773   20.967054   20.937731   21.063646   21.360701   \n",
              "2    18.171890   17.033098   16.999682   17.095999   17.076449   17.867253   \n",
              "3    21.039030   20.317165   20.217898   20.073852   19.794505   20.433907   \n",
              "4    25.700632   23.629122   21.742750   19.861718   18.810375   25.859229   \n",
              "\n",
              "    petroMag_g  petroMag_r  petroMag_i  ...  petroMag_g-i  petroMag_g-z  \\\n",
              "id                                      ...                               \n",
              "0    21.431636   21.478312   21.145409  ...      0.286226      1.009190   \n",
              "1    20.778968   20.889705   20.639812  ...      0.139156      0.132308   \n",
              "2    16.738784   16.688874   16.744210  ...     -0.005426     -0.069222   \n",
              "3    19.993727   19.985531   19.750917  ...      0.242810      0.538610   \n",
              "4    22.426929   21.673551   19.610012  ...      2.816917      4.050788   \n",
              "\n",
              "    petroMag_r-i  petroMag_r-z  petroMag_i-z  modelMag_u-g  modelMag_u-r  \\\n",
              "id                                                                         \n",
              "0       0.332902      1.055866      0.722964      1.283708      1.385054   \n",
              "1       0.249893      0.243045     -0.006847      0.734428      0.739030   \n",
              "2      -0.055336     -0.119132     -0.063796      1.120628      1.176814   \n",
              "3       0.234614      0.530413      0.295800      0.769012      0.880913   \n",
              "4       2.063539      3.297411      1.233871      1.729059      3.401710   \n",
              "\n",
              "    modelMag_u-i  modelMag_u-z  modelMag_g-r  modelMag_g-i  modelMag_g-z  \\\n",
              "id                                                                         \n",
              "0       1.728637      1.601901      0.101347      0.444929      0.318194   \n",
              "1       0.799566      0.980641      0.004602      0.065138      0.246213   \n",
              "2       1.157885      1.129134      0.056186      0.037257      0.008506   \n",
              "3       1.012598      1.217856      0.111901      0.243586      0.448844   \n",
              "4       5.389723      6.501398      1.672651      3.660663      4.772338   \n",
              "\n",
              "    modelMag_r-i  modelMag_r-z  modelMag_i-z  \n",
              "id                                            \n",
              "0       0.343582      0.216847     -0.126735  \n",
              "1       0.060537      0.241611      0.181074  \n",
              "2      -0.018929     -0.047680     -0.028751  \n",
              "3       0.131685      0.336943      0.205258  \n",
              "4       1.988012      3.099688      1.111675  \n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiMDAd9ZIdID",
        "colab_type": "code",
        "outputId": "12d98402-8620-4350-ba7c-1e0ef430af47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "train_X.columns"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['fiberID', 'psfMag_u', 'psfMag_g', 'psfMag_r', 'psfMag_i', 'psfMag_z',\n",
              "       'fiberMag_u', 'fiberMag_g', 'fiberMag_r', 'fiberMag_i', 'fiberMag_z',\n",
              "       'petroMag_u', 'petroMag_g', 'petroMag_r', 'petroMag_i', 'petroMag_z',\n",
              "       'modelMag_u', 'modelMag_g', 'modelMag_r', 'modelMag_i', 'modelMag_z',\n",
              "       'psfMag_u-g', 'psfMag_u-r', 'psfMag_u-i', 'psfMag_u-z', 'psfMag_g-r',\n",
              "       'psfMag_g-i', 'psfMag_g-z', 'psfMag_r-i', 'psfMag_r-z', 'psfMag_i-z',\n",
              "       'fiberMag_u-g', 'fiberMag_u-r', 'fiberMag_u-i', 'fiberMag_u-z',\n",
              "       'fiberMag_g-r', 'fiberMag_g-i', 'fiberMag_g-z', 'fiberMag_r-i',\n",
              "       'fiberMag_r-z', 'fiberMag_i-z', 'petroMag_u-g', 'petroMag_u-r',\n",
              "       'petroMag_u-i', 'petroMag_u-z', 'petroMag_g-r', 'petroMag_g-i',\n",
              "       'petroMag_g-z', 'petroMag_r-i', 'petroMag_r-z', 'petroMag_i-z',\n",
              "       'modelMag_u-g', 'modelMag_u-r', 'modelMag_u-i', 'modelMag_u-z',\n",
              "       'modelMag_g-r', 'modelMag_g-i', 'modelMag_g-z', 'modelMag_r-i',\n",
              "       'modelMag_r-z', 'modelMag_i-z'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sax8LG7lIdIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## u-g, g-r, r-i, i-z 만 남기는게 좋지 않을까????\n",
        "train_X.drop(['psfMag_u-r','psfMag_u-i','psfMag_u-z','psfMag_g-i','psfMag_g-z','psfMag_r-z',\n",
        "             'fiberMag_u-r','fiberMag_u-i','fiberMag_u-z','fiberMag_g-i','fiberMag_g-z','fiberMag_r-z',\n",
        "             'petroMag_u-r','petroMag_u-i','petroMag_u-z','petroMag_g-i','petroMag_g-z','petroMag_r-z',\n",
        "             'modelMag_u-r','modelMag_u-i','modelMag_u-z','modelMag_g-i','modelMag_g-z','modelMag_r-z'] ,axis=1, inplace=True)\n",
        "\n",
        "test_X.drop(['psfMag_u-r','psfMag_u-i','psfMag_u-z','psfMag_g-i','psfMag_g-z','psfMag_r-z',\n",
        "             'fiberMag_u-r','fiberMag_u-i','fiberMag_u-z','fiberMag_g-i','fiberMag_g-z','fiberMag_r-z',\n",
        "             'petroMag_u-r','petroMag_u-i','petroMag_u-z','petroMag_g-i','petroMag_g-z','petroMag_r-z',\n",
        "         'modelMag_u-r','modelMag_u-i','modelMag_u-z','modelMag_g-i','modelMag_g-z','modelMag_r-z'] ,axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6pvdT1rIdIP",
        "colab_type": "code",
        "outputId": "d0159417-5b3d-4e3a-9d0f-3b944f306d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_X.shape, test_X.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((199770, 37), (10009, 37))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUQNSQmmN5MR",
        "colab_type": "code",
        "outputId": "b38a9df9-7952-4f4e-f49b-59928a7d483e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "train_X.columns"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['fiberID', 'psfMag_u', 'psfMag_g', 'psfMag_r', 'psfMag_i', 'psfMag_z',\n",
              "       'fiberMag_u', 'fiberMag_g', 'fiberMag_r', 'fiberMag_i', 'fiberMag_z',\n",
              "       'petroMag_u', 'petroMag_g', 'petroMag_r', 'petroMag_i', 'petroMag_z',\n",
              "       'modelMag_u', 'modelMag_g', 'modelMag_r', 'modelMag_i', 'modelMag_z',\n",
              "       'psfMag_u-g', 'psfMag_g-r', 'psfMag_r-i', 'psfMag_i-z', 'fiberMag_u-g',\n",
              "       'fiberMag_g-r', 'fiberMag_r-i', 'fiberMag_i-z', 'petroMag_u-g',\n",
              "       'petroMag_g-r', 'petroMag_r-i', 'petroMag_i-z', 'modelMag_u-g',\n",
              "       'modelMag_g-r', 'modelMag_r-i', 'modelMag_i-z'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkoryQIdHqUq",
        "colab_type": "text"
      },
      "source": [
        "fr_hot = OneHotEncoder()\n",
        "fiber_train= fr_hot.fit_transform(train_id.values.reshape(-1,1))\n",
        "fiber_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwqJJUOxOPIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## fiber 관련변수 전부 제거\n",
        "train_X.drop(['fiberID','fiberMag_u', 'fiberMag_g', 'fiberMag_r', 'fiberMag_i', 'fiberMag_z',\n",
        "              'fiberMag_u-g','fiberMag_g-r', 'fiberMag_r-i', 'fiberMag_i-z', ], axis=1, inplace=True)\n",
        "\n",
        "test_X.drop(['fiberID','fiberMag_u', 'fiberMag_g', 'fiberMag_r', 'fiberMag_i', 'fiberMag_z',\n",
        "              'fiberMag_u-g','fiberMag_g-r', 'fiberMag_r-i', 'fiberMag_i-z', ], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZVRWqedIdIa",
        "colab_type": "code",
        "outputId": "7bb9b0b8-a83c-4cbd-e770-0b3ef1a3e75f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_X.shape, test_X.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((199770, 27), (10009, 27))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_Nds360IdIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.3, random_state=42,stratify = train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fCs6Z2zIdIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "o_hot = OneHotEncoder()\n",
        "y_train= o_hot.fit_transform(y_train.values.reshape(-1,1))\n",
        "y_train = y_train.toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2mwvZZ0tIdIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "o2_hot = OneHotEncoder()\n",
        "y_test= o2_hot.fit_transform(y_test.values.reshape(-1,1))\n",
        "y_test = y_test.toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "C8DaauwYIdIr",
        "colab_type": "code",
        "outputId": "d277c1db-3188-414e-827b-8893fe961df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train.shape, y_test.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((139839, 19), (59931, 19))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pg79AzKhIdIu",
        "colab_type": "code",
        "outputId": "f37c13ce-04b8-4d1a-a280-9e67740f4aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train = X_train.values\n",
        "X_train.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(139839, 27)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8eI0u2nIdI0",
        "colab_type": "code",
        "outputId": "de8ee773-917d-4883-d0aa-af3540485218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test = X_test.values\n",
        "X_test.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59931, 27)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "cWbu6unjIdI5",
        "colab_type": "code",
        "outputId": "4df6653c-f9bd-4fe7-cd3d-9f7766ee913f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "model = Sequential()\n",
        "model.add(Dense(256*4, activation='elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(256*3, activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(256*2, activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(256*1, activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128, activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(120, activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='elu'))\n",
        "model.add(Dense(32, activation='elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(19, activation='softmax'))\n",
        "\n",
        "\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "## 얼리스타핑\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
        "\n",
        "##최적모델 기억\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.00005), metrics=['accuracy','categorical_crossentropy'])\n",
        "\n",
        "history=model.fit(X_train, y_train, validation_data=(X_test, y_test) ,batch_size=200, epochs=2000, verbose=1,\n",
        "                 callbacks=[es, mc])\n",
        "### validation_data를 쓰면 test 데이터가 따로있을때 직접 넣을 수 있다.\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 139839 samples, validate on 59931 samples\n",
            "Epoch 1/2000\n",
            "139839/139839 [==============================] - 9s 64us/step - loss: 1.4249 - acc: 0.6396 - categorical_crossentropy: 1.4249 - val_loss: 0.8546 - val_acc: 0.7693 - val_categorical_crossentropy: 0.8546\n",
            "Epoch 2/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.9560 - acc: 0.7483 - categorical_crossentropy: 0.9560 - val_loss: 0.7100 - val_acc: 0.7975 - val_categorical_crossentropy: 0.7100\n",
            "Epoch 3/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.8242 - acc: 0.7704 - categorical_crossentropy: 0.8242 - val_loss: 0.6709 - val_acc: 0.8017 - val_categorical_crossentropy: 0.6709\n",
            "Epoch 4/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.7497 - acc: 0.7848 - categorical_crossentropy: 0.7497 - val_loss: 0.6029 - val_acc: 0.8231 - val_categorical_crossentropy: 0.6029\n",
            "Epoch 5/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.7049 - acc: 0.7914 - categorical_crossentropy: 0.7049 - val_loss: 0.5808 - val_acc: 0.8291 - val_categorical_crossentropy: 0.5808\n",
            "Epoch 6/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.6666 - acc: 0.8004 - categorical_crossentropy: 0.6666 - val_loss: 0.5833 - val_acc: 0.8187 - val_categorical_crossentropy: 0.5833\n",
            "Epoch 7/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.6424 - acc: 0.8033 - categorical_crossentropy: 0.6424 - val_loss: 0.5702 - val_acc: 0.8204 - val_categorical_crossentropy: 0.5702\n",
            "Epoch 8/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.6228 - acc: 0.8065 - categorical_crossentropy: 0.6228 - val_loss: 0.5665 - val_acc: 0.8163 - val_categorical_crossentropy: 0.5665\n",
            "Epoch 9/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.6057 - acc: 0.8102 - categorical_crossentropy: 0.6057 - val_loss: 0.5382 - val_acc: 0.8299 - val_categorical_crossentropy: 0.5382\n",
            "Epoch 10/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.5941 - acc: 0.8108 - categorical_crossentropy: 0.5941 - val_loss: 0.5053 - val_acc: 0.8340 - val_categorical_crossentropy: 0.5053\n",
            "Epoch 11/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.5830 - acc: 0.8140 - categorical_crossentropy: 0.5830 - val_loss: 0.5054 - val_acc: 0.8319 - val_categorical_crossentropy: 0.5054\n",
            "Epoch 12/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.5709 - acc: 0.8164 - categorical_crossentropy: 0.5709 - val_loss: 0.4867 - val_acc: 0.8380 - val_categorical_crossentropy: 0.4867\n",
            "Epoch 13/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.5628 - acc: 0.8184 - categorical_crossentropy: 0.5628 - val_loss: 0.5232 - val_acc: 0.8276 - val_categorical_crossentropy: 0.5232\n",
            "Epoch 14/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.5579 - acc: 0.8193 - categorical_crossentropy: 0.5579 - val_loss: 0.4923 - val_acc: 0.8390 - val_categorical_crossentropy: 0.4923\n",
            "Epoch 15/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.5548 - acc: 0.8192 - categorical_crossentropy: 0.5548 - val_loss: 0.4926 - val_acc: 0.8363 - val_categorical_crossentropy: 0.4926\n",
            "Epoch 16/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.5465 - acc: 0.8207 - categorical_crossentropy: 0.5465 - val_loss: 0.4752 - val_acc: 0.8440 - val_categorical_crossentropy: 0.4752\n",
            "Epoch 17/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.5409 - acc: 0.8222 - categorical_crossentropy: 0.5409 - val_loss: 0.5016 - val_acc: 0.8341 - val_categorical_crossentropy: 0.5016\n",
            "Epoch 18/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.5349 - acc: 0.8230 - categorical_crossentropy: 0.5349 - val_loss: 0.4939 - val_acc: 0.8347 - val_categorical_crossentropy: 0.4939\n",
            "Epoch 19/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.5325 - acc: 0.8249 - categorical_crossentropy: 0.5325 - val_loss: 0.4710 - val_acc: 0.8429 - val_categorical_crossentropy: 0.4710\n",
            "Epoch 20/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.5265 - acc: 0.8259 - categorical_crossentropy: 0.5265 - val_loss: 0.4565 - val_acc: 0.8453 - val_categorical_crossentropy: 0.4565\n",
            "Epoch 21/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.5236 - acc: 0.8266 - categorical_crossentropy: 0.5236 - val_loss: 0.5046 - val_acc: 0.8336 - val_categorical_crossentropy: 0.5046\n",
            "Epoch 22/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.5216 - acc: 0.8264 - categorical_crossentropy: 0.5216 - val_loss: 0.5097 - val_acc: 0.8282 - val_categorical_crossentropy: 0.5097\n",
            "Epoch 23/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.5181 - acc: 0.8280 - categorical_crossentropy: 0.5181 - val_loss: 0.4679 - val_acc: 0.8441 - val_categorical_crossentropy: 0.4679\n",
            "Epoch 24/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.5162 - acc: 0.8284 - categorical_crossentropy: 0.5162 - val_loss: 0.4609 - val_acc: 0.8433 - val_categorical_crossentropy: 0.4609\n",
            "Epoch 25/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.5119 - acc: 0.8296 - categorical_crossentropy: 0.5119 - val_loss: 0.4423 - val_acc: 0.8506 - val_categorical_crossentropy: 0.4423\n",
            "Epoch 26/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.5091 - acc: 0.8306 - categorical_crossentropy: 0.5091 - val_loss: 0.4775 - val_acc: 0.8394 - val_categorical_crossentropy: 0.4775\n",
            "Epoch 27/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.5063 - acc: 0.8315 - categorical_crossentropy: 0.5063 - val_loss: 0.4551 - val_acc: 0.8464 - val_categorical_crossentropy: 0.4551\n",
            "Epoch 28/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.5030 - acc: 0.8316 - categorical_crossentropy: 0.5030 - val_loss: 0.4644 - val_acc: 0.8439 - val_categorical_crossentropy: 0.4644\n",
            "Epoch 29/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.5014 - acc: 0.8322 - categorical_crossentropy: 0.5014 - val_loss: 0.6451 - val_acc: 0.7665 - val_categorical_crossentropy: 0.6451\n",
            "Epoch 30/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4975 - acc: 0.8341 - categorical_crossentropy: 0.4975 - val_loss: 0.4386 - val_acc: 0.8516 - val_categorical_crossentropy: 0.4386\n",
            "Epoch 31/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4976 - acc: 0.8332 - categorical_crossentropy: 0.4976 - val_loss: 0.4576 - val_acc: 0.8471 - val_categorical_crossentropy: 0.4576\n",
            "Epoch 32/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4963 - acc: 0.8337 - categorical_crossentropy: 0.4963 - val_loss: 0.4627 - val_acc: 0.8414 - val_categorical_crossentropy: 0.4627\n",
            "Epoch 33/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4939 - acc: 0.8339 - categorical_crossentropy: 0.4939 - val_loss: 0.4525 - val_acc: 0.8446 - val_categorical_crossentropy: 0.4525\n",
            "Epoch 34/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4950 - acc: 0.8336 - categorical_crossentropy: 0.4950 - val_loss: 0.4505 - val_acc: 0.8448 - val_categorical_crossentropy: 0.4505\n",
            "Epoch 35/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4896 - acc: 0.8351 - categorical_crossentropy: 0.4896 - val_loss: 0.4442 - val_acc: 0.8453 - val_categorical_crossentropy: 0.4442\n",
            "Epoch 36/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4894 - acc: 0.8356 - categorical_crossentropy: 0.4894 - val_loss: 0.4485 - val_acc: 0.8420 - val_categorical_crossentropy: 0.4485\n",
            "Epoch 37/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4859 - acc: 0.8362 - categorical_crossentropy: 0.4859 - val_loss: 0.4926 - val_acc: 0.8351 - val_categorical_crossentropy: 0.4926\n",
            "Epoch 38/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4867 - acc: 0.8352 - categorical_crossentropy: 0.4867 - val_loss: 0.4394 - val_acc: 0.8511 - val_categorical_crossentropy: 0.4394\n",
            "Epoch 39/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4867 - acc: 0.8356 - categorical_crossentropy: 0.4867 - val_loss: 0.4470 - val_acc: 0.8462 - val_categorical_crossentropy: 0.4470\n",
            "Epoch 40/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4824 - acc: 0.8378 - categorical_crossentropy: 0.4824 - val_loss: 0.4411 - val_acc: 0.8452 - val_categorical_crossentropy: 0.4411\n",
            "Epoch 41/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4812 - acc: 0.8373 - categorical_crossentropy: 0.4812 - val_loss: 0.4380 - val_acc: 0.8511 - val_categorical_crossentropy: 0.4380\n",
            "Epoch 42/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4792 - acc: 0.8378 - categorical_crossentropy: 0.4792 - val_loss: 0.4572 - val_acc: 0.8383 - val_categorical_crossentropy: 0.4572\n",
            "Epoch 43/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4820 - acc: 0.8375 - categorical_crossentropy: 0.4820 - val_loss: 0.4656 - val_acc: 0.8398 - val_categorical_crossentropy: 0.4656\n",
            "Epoch 44/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4815 - acc: 0.8368 - categorical_crossentropy: 0.4815 - val_loss: 0.4422 - val_acc: 0.8446 - val_categorical_crossentropy: 0.4422\n",
            "Epoch 45/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4768 - acc: 0.8383 - categorical_crossentropy: 0.4768 - val_loss: 0.4341 - val_acc: 0.8495 - val_categorical_crossentropy: 0.4341\n",
            "Epoch 46/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4772 - acc: 0.8394 - categorical_crossentropy: 0.4772 - val_loss: 0.4265 - val_acc: 0.8526 - val_categorical_crossentropy: 0.4265\n",
            "Epoch 47/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4745 - acc: 0.8387 - categorical_crossentropy: 0.4745 - val_loss: 0.4250 - val_acc: 0.8543 - val_categorical_crossentropy: 0.4250\n",
            "Epoch 48/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4743 - acc: 0.8389 - categorical_crossentropy: 0.4743 - val_loss: 0.4376 - val_acc: 0.8535 - val_categorical_crossentropy: 0.4376\n",
            "Epoch 49/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4739 - acc: 0.8399 - categorical_crossentropy: 0.4739 - val_loss: 0.4397 - val_acc: 0.8468 - val_categorical_crossentropy: 0.4397\n",
            "Epoch 50/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4723 - acc: 0.8390 - categorical_crossentropy: 0.4723 - val_loss: 0.4995 - val_acc: 0.8220 - val_categorical_crossentropy: 0.4995\n",
            "Epoch 51/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4705 - acc: 0.8407 - categorical_crossentropy: 0.4705 - val_loss: 0.4279 - val_acc: 0.8521 - val_categorical_crossentropy: 0.4279\n",
            "Epoch 52/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4718 - acc: 0.8398 - categorical_crossentropy: 0.4718 - val_loss: 0.4229 - val_acc: 0.8562 - val_categorical_crossentropy: 0.4229\n",
            "Epoch 53/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4690 - acc: 0.8400 - categorical_crossentropy: 0.4690 - val_loss: 0.4266 - val_acc: 0.8511 - val_categorical_crossentropy: 0.4266\n",
            "Epoch 54/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4691 - acc: 0.8409 - categorical_crossentropy: 0.4691 - val_loss: 0.4783 - val_acc: 0.8389 - val_categorical_crossentropy: 0.4783\n",
            "Epoch 55/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4676 - acc: 0.8404 - categorical_crossentropy: 0.4676 - val_loss: 0.4578 - val_acc: 0.8384 - val_categorical_crossentropy: 0.4578\n",
            "Epoch 56/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4659 - acc: 0.8418 - categorical_crossentropy: 0.4659 - val_loss: 0.5562 - val_acc: 0.8111 - val_categorical_crossentropy: 0.5562\n",
            "Epoch 57/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4656 - acc: 0.8410 - categorical_crossentropy: 0.4656 - val_loss: 0.4555 - val_acc: 0.8447 - val_categorical_crossentropy: 0.4555\n",
            "Epoch 58/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4649 - acc: 0.8421 - categorical_crossentropy: 0.4649 - val_loss: 0.5018 - val_acc: 0.8185 - val_categorical_crossentropy: 0.5018\n",
            "Epoch 59/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4640 - acc: 0.8415 - categorical_crossentropy: 0.4640 - val_loss: 0.4178 - val_acc: 0.8560 - val_categorical_crossentropy: 0.4178\n",
            "Epoch 60/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4629 - acc: 0.8419 - categorical_crossentropy: 0.4629 - val_loss: 0.4301 - val_acc: 0.8523 - val_categorical_crossentropy: 0.4301\n",
            "Epoch 61/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4639 - acc: 0.8417 - categorical_crossentropy: 0.4639 - val_loss: 0.4388 - val_acc: 0.8497 - val_categorical_crossentropy: 0.4388\n",
            "Epoch 62/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4626 - acc: 0.8427 - categorical_crossentropy: 0.4626 - val_loss: 0.4670 - val_acc: 0.8445 - val_categorical_crossentropy: 0.4670\n",
            "Epoch 63/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4595 - acc: 0.8430 - categorical_crossentropy: 0.4595 - val_loss: 0.5990 - val_acc: 0.7886 - val_categorical_crossentropy: 0.5990\n",
            "Epoch 64/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4613 - acc: 0.8429 - categorical_crossentropy: 0.4613 - val_loss: 0.4393 - val_acc: 0.8526 - val_categorical_crossentropy: 0.4393\n",
            "Epoch 65/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4591 - acc: 0.8428 - categorical_crossentropy: 0.4591 - val_loss: 0.5148 - val_acc: 0.8214 - val_categorical_crossentropy: 0.5148\n",
            "Epoch 66/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4583 - acc: 0.8438 - categorical_crossentropy: 0.4583 - val_loss: 0.4229 - val_acc: 0.8536 - val_categorical_crossentropy: 0.4229\n",
            "Epoch 67/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4568 - acc: 0.8444 - categorical_crossentropy: 0.4568 - val_loss: 0.4402 - val_acc: 0.8443 - val_categorical_crossentropy: 0.4402\n",
            "Epoch 68/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4596 - acc: 0.8438 - categorical_crossentropy: 0.4596 - val_loss: 0.4244 - val_acc: 0.8551 - val_categorical_crossentropy: 0.4244\n",
            "Epoch 69/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4577 - acc: 0.8437 - categorical_crossentropy: 0.4577 - val_loss: 0.4291 - val_acc: 0.8536 - val_categorical_crossentropy: 0.4291\n",
            "Epoch 70/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4568 - acc: 0.8438 - categorical_crossentropy: 0.4568 - val_loss: 0.4359 - val_acc: 0.8493 - val_categorical_crossentropy: 0.4359\n",
            "Epoch 71/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4572 - acc: 0.8437 - categorical_crossentropy: 0.4572 - val_loss: 0.4324 - val_acc: 0.8515 - val_categorical_crossentropy: 0.4324\n",
            "Epoch 72/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4562 - acc: 0.8443 - categorical_crossentropy: 0.4562 - val_loss: 0.4118 - val_acc: 0.8593 - val_categorical_crossentropy: 0.4118\n",
            "Epoch 73/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4545 - acc: 0.8444 - categorical_crossentropy: 0.4545 - val_loss: 0.4226 - val_acc: 0.8567 - val_categorical_crossentropy: 0.4226\n",
            "Epoch 74/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4539 - acc: 0.8458 - categorical_crossentropy: 0.4539 - val_loss: 0.4378 - val_acc: 0.8458 - val_categorical_crossentropy: 0.4378\n",
            "Epoch 75/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4536 - acc: 0.8445 - categorical_crossentropy: 0.4536 - val_loss: 0.4255 - val_acc: 0.8544 - val_categorical_crossentropy: 0.4255\n",
            "Epoch 76/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4536 - acc: 0.8444 - categorical_crossentropy: 0.4536 - val_loss: 0.4723 - val_acc: 0.8329 - val_categorical_crossentropy: 0.4723\n",
            "Epoch 77/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4508 - acc: 0.8458 - categorical_crossentropy: 0.4508 - val_loss: 0.4038 - val_acc: 0.8604 - val_categorical_crossentropy: 0.4038\n",
            "Epoch 78/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4502 - acc: 0.8463 - categorical_crossentropy: 0.4502 - val_loss: 0.4320 - val_acc: 0.8472 - val_categorical_crossentropy: 0.4320\n",
            "Epoch 79/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4516 - acc: 0.8464 - categorical_crossentropy: 0.4516 - val_loss: 0.4500 - val_acc: 0.8448 - val_categorical_crossentropy: 0.4500\n",
            "Epoch 80/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4506 - acc: 0.8460 - categorical_crossentropy: 0.4506 - val_loss: 0.4290 - val_acc: 0.8510 - val_categorical_crossentropy: 0.4290\n",
            "Epoch 81/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4501 - acc: 0.8453 - categorical_crossentropy: 0.4501 - val_loss: 0.4411 - val_acc: 0.8482 - val_categorical_crossentropy: 0.4411\n",
            "Epoch 82/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4491 - acc: 0.8464 - categorical_crossentropy: 0.4491 - val_loss: 0.4276 - val_acc: 0.8501 - val_categorical_crossentropy: 0.4276\n",
            "Epoch 83/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4476 - acc: 0.8471 - categorical_crossentropy: 0.4476 - val_loss: 0.4657 - val_acc: 0.8411 - val_categorical_crossentropy: 0.4657\n",
            "Epoch 84/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4485 - acc: 0.8467 - categorical_crossentropy: 0.4485 - val_loss: 0.4340 - val_acc: 0.8504 - val_categorical_crossentropy: 0.4340\n",
            "Epoch 85/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4485 - acc: 0.8459 - categorical_crossentropy: 0.4485 - val_loss: 0.4369 - val_acc: 0.8496 - val_categorical_crossentropy: 0.4369\n",
            "Epoch 86/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4469 - acc: 0.8473 - categorical_crossentropy: 0.4469 - val_loss: 0.4931 - val_acc: 0.8261 - val_categorical_crossentropy: 0.4931\n",
            "Epoch 87/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4484 - acc: 0.8465 - categorical_crossentropy: 0.4484 - val_loss: 0.4023 - val_acc: 0.8609 - val_categorical_crossentropy: 0.4023\n",
            "Epoch 88/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4445 - acc: 0.8475 - categorical_crossentropy: 0.4445 - val_loss: 0.4391 - val_acc: 0.8507 - val_categorical_crossentropy: 0.4391\n",
            "Epoch 89/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4437 - acc: 0.8488 - categorical_crossentropy: 0.4437 - val_loss: 0.8067 - val_acc: 0.7192 - val_categorical_crossentropy: 0.8067\n",
            "Epoch 90/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4463 - acc: 0.8471 - categorical_crossentropy: 0.4463 - val_loss: 0.4011 - val_acc: 0.8624 - val_categorical_crossentropy: 0.4011\n",
            "Epoch 91/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4470 - acc: 0.8464 - categorical_crossentropy: 0.4470 - val_loss: 0.4656 - val_acc: 0.8480 - val_categorical_crossentropy: 0.4656\n",
            "Epoch 92/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4458 - acc: 0.8475 - categorical_crossentropy: 0.4458 - val_loss: 0.4192 - val_acc: 0.8547 - val_categorical_crossentropy: 0.4192\n",
            "Epoch 93/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4444 - acc: 0.8479 - categorical_crossentropy: 0.4444 - val_loss: 0.4061 - val_acc: 0.8596 - val_categorical_crossentropy: 0.4061\n",
            "Epoch 94/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4434 - acc: 0.8485 - categorical_crossentropy: 0.4434 - val_loss: 0.4493 - val_acc: 0.8537 - val_categorical_crossentropy: 0.4493\n",
            "Epoch 95/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4439 - acc: 0.8473 - categorical_crossentropy: 0.4439 - val_loss: 0.4097 - val_acc: 0.8559 - val_categorical_crossentropy: 0.4097\n",
            "Epoch 96/2000\n",
            "139839/139839 [==============================] - 7s 49us/step - loss: 0.4439 - acc: 0.8476 - categorical_crossentropy: 0.4439 - val_loss: 0.4235 - val_acc: 0.8497 - val_categorical_crossentropy: 0.4235\n",
            "Epoch 97/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4429 - acc: 0.8478 - categorical_crossentropy: 0.4429 - val_loss: 0.4204 - val_acc: 0.8543 - val_categorical_crossentropy: 0.4204\n",
            "Epoch 98/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4411 - acc: 0.8491 - categorical_crossentropy: 0.4411 - val_loss: 0.4174 - val_acc: 0.8545 - val_categorical_crossentropy: 0.4174\n",
            "Epoch 99/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4435 - acc: 0.8481 - categorical_crossentropy: 0.4435 - val_loss: 0.3987 - val_acc: 0.8601 - val_categorical_crossentropy: 0.3987\n",
            "Epoch 100/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4416 - acc: 0.8487 - categorical_crossentropy: 0.4416 - val_loss: 0.4057 - val_acc: 0.8613 - val_categorical_crossentropy: 0.4057\n",
            "Epoch 101/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4422 - acc: 0.8486 - categorical_crossentropy: 0.4422 - val_loss: 0.5321 - val_acc: 0.8062 - val_categorical_crossentropy: 0.5321\n",
            "Epoch 102/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4404 - acc: 0.8490 - categorical_crossentropy: 0.4404 - val_loss: 0.4029 - val_acc: 0.8616 - val_categorical_crossentropy: 0.4029\n",
            "Epoch 103/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4406 - acc: 0.8483 - categorical_crossentropy: 0.4406 - val_loss: 0.5078 - val_acc: 0.8416 - val_categorical_crossentropy: 0.5078\n",
            "Epoch 104/2000\n",
            "139839/139839 [==============================] - 7s 46us/step - loss: 0.4402 - acc: 0.8491 - categorical_crossentropy: 0.4402 - val_loss: 0.4044 - val_acc: 0.8593 - val_categorical_crossentropy: 0.4044\n",
            "Epoch 105/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4402 - acc: 0.8485 - categorical_crossentropy: 0.4402 - val_loss: 0.4789 - val_acc: 0.8341 - val_categorical_crossentropy: 0.4789\n",
            "Epoch 106/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4387 - acc: 0.8490 - categorical_crossentropy: 0.4387 - val_loss: 0.4179 - val_acc: 0.8574 - val_categorical_crossentropy: 0.4179\n",
            "Epoch 107/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4380 - acc: 0.8500 - categorical_crossentropy: 0.4380 - val_loss: 0.4521 - val_acc: 0.8496 - val_categorical_crossentropy: 0.4521\n",
            "Epoch 108/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4369 - acc: 0.8503 - categorical_crossentropy: 0.4369 - val_loss: 0.5319 - val_acc: 0.8079 - val_categorical_crossentropy: 0.5319\n",
            "Epoch 109/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4357 - acc: 0.8500 - categorical_crossentropy: 0.4357 - val_loss: 0.4150 - val_acc: 0.8538 - val_categorical_crossentropy: 0.4150\n",
            "Epoch 110/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4370 - acc: 0.8500 - categorical_crossentropy: 0.4370 - val_loss: 0.4481 - val_acc: 0.8396 - val_categorical_crossentropy: 0.4481\n",
            "Epoch 111/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4364 - acc: 0.8492 - categorical_crossentropy: 0.4364 - val_loss: 0.4340 - val_acc: 0.8521 - val_categorical_crossentropy: 0.4340\n",
            "Epoch 112/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4356 - acc: 0.8502 - categorical_crossentropy: 0.4356 - val_loss: 0.7528 - val_acc: 0.7225 - val_categorical_crossentropy: 0.7528\n",
            "Epoch 113/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4362 - acc: 0.8502 - categorical_crossentropy: 0.4362 - val_loss: 0.4288 - val_acc: 0.8514 - val_categorical_crossentropy: 0.4288\n",
            "Epoch 114/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4378 - acc: 0.8490 - categorical_crossentropy: 0.4378 - val_loss: 0.4187 - val_acc: 0.8545 - val_categorical_crossentropy: 0.4187\n",
            "Epoch 115/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4377 - acc: 0.8498 - categorical_crossentropy: 0.4377 - val_loss: 0.4267 - val_acc: 0.8506 - val_categorical_crossentropy: 0.4267\n",
            "Epoch 116/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4349 - acc: 0.8502 - categorical_crossentropy: 0.4349 - val_loss: 0.5317 - val_acc: 0.8118 - val_categorical_crossentropy: 0.5317\n",
            "Epoch 117/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4359 - acc: 0.8501 - categorical_crossentropy: 0.4359 - val_loss: 0.4099 - val_acc: 0.8581 - val_categorical_crossentropy: 0.4099\n",
            "Epoch 118/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4343 - acc: 0.8514 - categorical_crossentropy: 0.4343 - val_loss: 0.4068 - val_acc: 0.8594 - val_categorical_crossentropy: 0.4068\n",
            "Epoch 119/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4352 - acc: 0.8507 - categorical_crossentropy: 0.4352 - val_loss: 0.4743 - val_acc: 0.8503 - val_categorical_crossentropy: 0.4743\n",
            "Epoch 120/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4358 - acc: 0.8499 - categorical_crossentropy: 0.4358 - val_loss: 0.3980 - val_acc: 0.8615 - val_categorical_crossentropy: 0.3980\n",
            "Epoch 121/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4368 - acc: 0.8491 - categorical_crossentropy: 0.4368 - val_loss: 0.4136 - val_acc: 0.8602 - val_categorical_crossentropy: 0.4136\n",
            "Epoch 122/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4343 - acc: 0.8509 - categorical_crossentropy: 0.4343 - val_loss: 0.4418 - val_acc: 0.8495 - val_categorical_crossentropy: 0.4418\n",
            "Epoch 123/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4318 - acc: 0.8508 - categorical_crossentropy: 0.4318 - val_loss: 0.4263 - val_acc: 0.8558 - val_categorical_crossentropy: 0.4263\n",
            "Epoch 124/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4338 - acc: 0.8506 - categorical_crossentropy: 0.4338 - val_loss: 0.4070 - val_acc: 0.8610 - val_categorical_crossentropy: 0.4070\n",
            "Epoch 125/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4345 - acc: 0.8500 - categorical_crossentropy: 0.4345 - val_loss: 0.4491 - val_acc: 0.8371 - val_categorical_crossentropy: 0.4491\n",
            "Epoch 126/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4326 - acc: 0.8510 - categorical_crossentropy: 0.4326 - val_loss: 0.4638 - val_acc: 0.8322 - val_categorical_crossentropy: 0.4638\n",
            "Epoch 127/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4317 - acc: 0.8517 - categorical_crossentropy: 0.4317 - val_loss: 0.4263 - val_acc: 0.8487 - val_categorical_crossentropy: 0.4263\n",
            "Epoch 128/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4322 - acc: 0.8511 - categorical_crossentropy: 0.4322 - val_loss: 0.4432 - val_acc: 0.8512 - val_categorical_crossentropy: 0.4432\n",
            "Epoch 129/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4312 - acc: 0.8509 - categorical_crossentropy: 0.4312 - val_loss: 0.4180 - val_acc: 0.8552 - val_categorical_crossentropy: 0.4180\n",
            "Epoch 130/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4310 - acc: 0.8517 - categorical_crossentropy: 0.4310 - val_loss: 0.4197 - val_acc: 0.8554 - val_categorical_crossentropy: 0.4197\n",
            "Epoch 131/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4302 - acc: 0.8513 - categorical_crossentropy: 0.4302 - val_loss: 0.5150 - val_acc: 0.8259 - val_categorical_crossentropy: 0.5150\n",
            "Epoch 132/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4301 - acc: 0.8513 - categorical_crossentropy: 0.4301 - val_loss: 0.4250 - val_acc: 0.8547 - val_categorical_crossentropy: 0.4250\n",
            "Epoch 133/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4320 - acc: 0.8508 - categorical_crossentropy: 0.4320 - val_loss: 0.3918 - val_acc: 0.8655 - val_categorical_crossentropy: 0.3918\n",
            "Epoch 134/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4308 - acc: 0.8513 - categorical_crossentropy: 0.4308 - val_loss: 0.3975 - val_acc: 0.8613 - val_categorical_crossentropy: 0.3975\n",
            "Epoch 135/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4308 - acc: 0.8520 - categorical_crossentropy: 0.4308 - val_loss: 0.3852 - val_acc: 0.8668 - val_categorical_crossentropy: 0.3852\n",
            "Epoch 136/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4297 - acc: 0.8520 - categorical_crossentropy: 0.4297 - val_loss: 0.4403 - val_acc: 0.8524 - val_categorical_crossentropy: 0.4403\n",
            "Epoch 137/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4314 - acc: 0.8508 - categorical_crossentropy: 0.4314 - val_loss: 0.3940 - val_acc: 0.8632 - val_categorical_crossentropy: 0.3940\n",
            "Epoch 138/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4316 - acc: 0.8512 - categorical_crossentropy: 0.4316 - val_loss: 0.4421 - val_acc: 0.8450 - val_categorical_crossentropy: 0.4421\n",
            "Epoch 139/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4304 - acc: 0.8520 - categorical_crossentropy: 0.4304 - val_loss: 0.4145 - val_acc: 0.8554 - val_categorical_crossentropy: 0.4145\n",
            "Epoch 140/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4286 - acc: 0.8519 - categorical_crossentropy: 0.4286 - val_loss: 0.4225 - val_acc: 0.8551 - val_categorical_crossentropy: 0.4225\n",
            "Epoch 141/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4303 - acc: 0.8521 - categorical_crossentropy: 0.4303 - val_loss: 0.3931 - val_acc: 0.8636 - val_categorical_crossentropy: 0.3931\n",
            "Epoch 142/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4290 - acc: 0.8510 - categorical_crossentropy: 0.4290 - val_loss: 0.3999 - val_acc: 0.8620 - val_categorical_crossentropy: 0.3999\n",
            "Epoch 143/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4274 - acc: 0.8526 - categorical_crossentropy: 0.4274 - val_loss: 0.4449 - val_acc: 0.8472 - val_categorical_crossentropy: 0.4449\n",
            "Epoch 144/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4283 - acc: 0.8520 - categorical_crossentropy: 0.4283 - val_loss: 0.4500 - val_acc: 0.8462 - val_categorical_crossentropy: 0.4500\n",
            "Epoch 145/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4257 - acc: 0.8530 - categorical_crossentropy: 0.4257 - val_loss: 0.4186 - val_acc: 0.8536 - val_categorical_crossentropy: 0.4186\n",
            "Epoch 146/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4274 - acc: 0.8525 - categorical_crossentropy: 0.4274 - val_loss: 0.4505 - val_acc: 0.8498 - val_categorical_crossentropy: 0.4505\n",
            "Epoch 147/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4272 - acc: 0.8523 - categorical_crossentropy: 0.4272 - val_loss: 0.4110 - val_acc: 0.8565 - val_categorical_crossentropy: 0.4110\n",
            "Epoch 148/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4272 - acc: 0.8524 - categorical_crossentropy: 0.4272 - val_loss: 0.4195 - val_acc: 0.8569 - val_categorical_crossentropy: 0.4195\n",
            "Epoch 149/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4260 - acc: 0.8529 - categorical_crossentropy: 0.4260 - val_loss: 0.4311 - val_acc: 0.8558 - val_categorical_crossentropy: 0.4311\n",
            "Epoch 150/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4261 - acc: 0.8532 - categorical_crossentropy: 0.4261 - val_loss: 0.3901 - val_acc: 0.8650 - val_categorical_crossentropy: 0.3901\n",
            "Epoch 151/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4257 - acc: 0.8529 - categorical_crossentropy: 0.4257 - val_loss: 0.4075 - val_acc: 0.8608 - val_categorical_crossentropy: 0.4075\n",
            "Epoch 152/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4268 - acc: 0.8524 - categorical_crossentropy: 0.4268 - val_loss: 0.4263 - val_acc: 0.8484 - val_categorical_crossentropy: 0.4263\n",
            "Epoch 153/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4236 - acc: 0.8530 - categorical_crossentropy: 0.4236 - val_loss: 0.4602 - val_acc: 0.8370 - val_categorical_crossentropy: 0.4602\n",
            "Epoch 154/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4266 - acc: 0.8529 - categorical_crossentropy: 0.4266 - val_loss: 0.4005 - val_acc: 0.8633 - val_categorical_crossentropy: 0.4005\n",
            "Epoch 155/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4271 - acc: 0.8528 - categorical_crossentropy: 0.4271 - val_loss: 0.4293 - val_acc: 0.8487 - val_categorical_crossentropy: 0.4293\n",
            "Epoch 156/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4270 - acc: 0.8523 - categorical_crossentropy: 0.4270 - val_loss: 0.4343 - val_acc: 0.8489 - val_categorical_crossentropy: 0.4343\n",
            "Epoch 157/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4248 - acc: 0.8532 - categorical_crossentropy: 0.4248 - val_loss: 0.4122 - val_acc: 0.8570 - val_categorical_crossentropy: 0.4122\n",
            "Epoch 158/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4236 - acc: 0.8530 - categorical_crossentropy: 0.4236 - val_loss: 0.4226 - val_acc: 0.8522 - val_categorical_crossentropy: 0.4226\n",
            "Epoch 159/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4228 - acc: 0.8549 - categorical_crossentropy: 0.4228 - val_loss: 0.4049 - val_acc: 0.8578 - val_categorical_crossentropy: 0.4049\n",
            "Epoch 160/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4225 - acc: 0.8533 - categorical_crossentropy: 0.4225 - val_loss: 0.4576 - val_acc: 0.8507 - val_categorical_crossentropy: 0.4576\n",
            "Epoch 161/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4228 - acc: 0.8540 - categorical_crossentropy: 0.4228 - val_loss: 0.4090 - val_acc: 0.8609 - val_categorical_crossentropy: 0.4090\n",
            "Epoch 162/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4240 - acc: 0.8538 - categorical_crossentropy: 0.4240 - val_loss: 0.4136 - val_acc: 0.8550 - val_categorical_crossentropy: 0.4136\n",
            "Epoch 163/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4214 - acc: 0.8544 - categorical_crossentropy: 0.4214 - val_loss: 0.4249 - val_acc: 0.8506 - val_categorical_crossentropy: 0.4249\n",
            "Epoch 164/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4237 - acc: 0.8533 - categorical_crossentropy: 0.4237 - val_loss: 0.3992 - val_acc: 0.8596 - val_categorical_crossentropy: 0.3992\n",
            "Epoch 165/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4229 - acc: 0.8532 - categorical_crossentropy: 0.4229 - val_loss: 0.3958 - val_acc: 0.8599 - val_categorical_crossentropy: 0.3958\n",
            "Epoch 166/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4229 - acc: 0.8538 - categorical_crossentropy: 0.4229 - val_loss: 0.4422 - val_acc: 0.8423 - val_categorical_crossentropy: 0.4422\n",
            "Epoch 167/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4228 - acc: 0.8542 - categorical_crossentropy: 0.4228 - val_loss: 0.3941 - val_acc: 0.8605 - val_categorical_crossentropy: 0.3941\n",
            "Epoch 168/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4208 - acc: 0.8547 - categorical_crossentropy: 0.4208 - val_loss: 0.4511 - val_acc: 0.8374 - val_categorical_crossentropy: 0.4511\n",
            "Epoch 169/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4203 - acc: 0.8547 - categorical_crossentropy: 0.4203 - val_loss: 0.3953 - val_acc: 0.8633 - val_categorical_crossentropy: 0.3953\n",
            "Epoch 170/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4214 - acc: 0.8541 - categorical_crossentropy: 0.4214 - val_loss: 0.4244 - val_acc: 0.8504 - val_categorical_crossentropy: 0.4244\n",
            "Epoch 171/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4202 - acc: 0.8551 - categorical_crossentropy: 0.4202 - val_loss: 0.4975 - val_acc: 0.8187 - val_categorical_crossentropy: 0.4975\n",
            "Epoch 172/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4198 - acc: 0.8542 - categorical_crossentropy: 0.4198 - val_loss: 0.3999 - val_acc: 0.8607 - val_categorical_crossentropy: 0.3999\n",
            "Epoch 173/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4196 - acc: 0.8553 - categorical_crossentropy: 0.4196 - val_loss: 0.4941 - val_acc: 0.8200 - val_categorical_crossentropy: 0.4941\n",
            "Epoch 174/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4197 - acc: 0.8545 - categorical_crossentropy: 0.4197 - val_loss: 0.4442 - val_acc: 0.8556 - val_categorical_crossentropy: 0.4442\n",
            "Epoch 175/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4190 - acc: 0.8550 - categorical_crossentropy: 0.4190 - val_loss: 0.3930 - val_acc: 0.8645 - val_categorical_crossentropy: 0.3930\n",
            "Epoch 176/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4191 - acc: 0.8555 - categorical_crossentropy: 0.4191 - val_loss: 0.4044 - val_acc: 0.8586 - val_categorical_crossentropy: 0.4044\n",
            "Epoch 177/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4195 - acc: 0.8546 - categorical_crossentropy: 0.4195 - val_loss: 0.4277 - val_acc: 0.8486 - val_categorical_crossentropy: 0.4277\n",
            "Epoch 178/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4198 - acc: 0.8548 - categorical_crossentropy: 0.4198 - val_loss: 0.4452 - val_acc: 0.8548 - val_categorical_crossentropy: 0.4452\n",
            "Epoch 179/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4187 - acc: 0.8549 - categorical_crossentropy: 0.4187 - val_loss: 0.3852 - val_acc: 0.8654 - val_categorical_crossentropy: 0.3852\n",
            "Epoch 180/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4219 - acc: 0.8534 - categorical_crossentropy: 0.4219 - val_loss: 0.4168 - val_acc: 0.8529 - val_categorical_crossentropy: 0.4168\n",
            "Epoch 181/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4195 - acc: 0.8541 - categorical_crossentropy: 0.4195 - val_loss: 0.4458 - val_acc: 0.8448 - val_categorical_crossentropy: 0.4458\n",
            "Epoch 182/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4181 - acc: 0.8553 - categorical_crossentropy: 0.4181 - val_loss: 0.3850 - val_acc: 0.8651 - val_categorical_crossentropy: 0.3850\n",
            "Epoch 183/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4202 - acc: 0.8550 - categorical_crossentropy: 0.4202 - val_loss: 0.4169 - val_acc: 0.8542 - val_categorical_crossentropy: 0.4169\n",
            "Epoch 184/2000\n",
            "139839/139839 [==============================] - 7s 46us/step - loss: 0.4192 - acc: 0.8558 - categorical_crossentropy: 0.4192 - val_loss: 0.4034 - val_acc: 0.8604 - val_categorical_crossentropy: 0.4034\n",
            "Epoch 185/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4194 - acc: 0.8546 - categorical_crossentropy: 0.4194 - val_loss: 0.4099 - val_acc: 0.8582 - val_categorical_crossentropy: 0.4099\n",
            "Epoch 186/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4191 - acc: 0.8543 - categorical_crossentropy: 0.4191 - val_loss: 0.4113 - val_acc: 0.8604 - val_categorical_crossentropy: 0.4113\n",
            "Epoch 187/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4179 - acc: 0.8557 - categorical_crossentropy: 0.4179 - val_loss: 0.3975 - val_acc: 0.8611 - val_categorical_crossentropy: 0.3975\n",
            "Epoch 188/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4177 - acc: 0.8551 - categorical_crossentropy: 0.4177 - val_loss: 0.4271 - val_acc: 0.8499 - val_categorical_crossentropy: 0.4271\n",
            "Epoch 189/2000\n",
            "139839/139839 [==============================] - 7s 50us/step - loss: 0.4185 - acc: 0.8558 - categorical_crossentropy: 0.4185 - val_loss: 0.4939 - val_acc: 0.8391 - val_categorical_crossentropy: 0.4939\n",
            "Epoch 190/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4177 - acc: 0.8550 - categorical_crossentropy: 0.4177 - val_loss: 0.4053 - val_acc: 0.8579 - val_categorical_crossentropy: 0.4053\n",
            "Epoch 191/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4192 - acc: 0.8547 - categorical_crossentropy: 0.4192 - val_loss: 0.4178 - val_acc: 0.8502 - val_categorical_crossentropy: 0.4178\n",
            "Epoch 192/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4172 - acc: 0.8558 - categorical_crossentropy: 0.4172 - val_loss: 0.4270 - val_acc: 0.8506 - val_categorical_crossentropy: 0.4270\n",
            "Epoch 193/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4189 - acc: 0.8550 - categorical_crossentropy: 0.4189 - val_loss: 0.4307 - val_acc: 0.8551 - val_categorical_crossentropy: 0.4307\n",
            "Epoch 194/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4185 - acc: 0.8549 - categorical_crossentropy: 0.4185 - val_loss: 0.4419 - val_acc: 0.8478 - val_categorical_crossentropy: 0.4419\n",
            "Epoch 195/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4153 - acc: 0.8561 - categorical_crossentropy: 0.4153 - val_loss: 0.4468 - val_acc: 0.8403 - val_categorical_crossentropy: 0.4468\n",
            "Epoch 196/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4177 - acc: 0.8552 - categorical_crossentropy: 0.4177 - val_loss: 0.3880 - val_acc: 0.8651 - val_categorical_crossentropy: 0.3880\n",
            "Epoch 197/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4165 - acc: 0.8558 - categorical_crossentropy: 0.4165 - val_loss: 0.4619 - val_acc: 0.8344 - val_categorical_crossentropy: 0.4619\n",
            "Epoch 198/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4151 - acc: 0.8557 - categorical_crossentropy: 0.4151 - val_loss: 0.3949 - val_acc: 0.8619 - val_categorical_crossentropy: 0.3949\n",
            "Epoch 199/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4168 - acc: 0.8556 - categorical_crossentropy: 0.4168 - val_loss: 0.4240 - val_acc: 0.8540 - val_categorical_crossentropy: 0.4240\n",
            "Epoch 200/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4154 - acc: 0.8553 - categorical_crossentropy: 0.4154 - val_loss: 0.6083 - val_acc: 0.7846 - val_categorical_crossentropy: 0.6083\n",
            "Epoch 201/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4169 - acc: 0.8554 - categorical_crossentropy: 0.4169 - val_loss: 0.6107 - val_acc: 0.7792 - val_categorical_crossentropy: 0.6107\n",
            "Epoch 202/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4165 - acc: 0.8554 - categorical_crossentropy: 0.4165 - val_loss: 0.4345 - val_acc: 0.8446 - val_categorical_crossentropy: 0.4345\n",
            "Epoch 203/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4162 - acc: 0.8555 - categorical_crossentropy: 0.4162 - val_loss: 0.4376 - val_acc: 0.8459 - val_categorical_crossentropy: 0.4376\n",
            "Epoch 204/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4140 - acc: 0.8562 - categorical_crossentropy: 0.4140 - val_loss: 0.3921 - val_acc: 0.8633 - val_categorical_crossentropy: 0.3921\n",
            "Epoch 205/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4148 - acc: 0.8564 - categorical_crossentropy: 0.4148 - val_loss: 0.4067 - val_acc: 0.8586 - val_categorical_crossentropy: 0.4067\n",
            "Epoch 206/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4160 - acc: 0.8556 - categorical_crossentropy: 0.4160 - val_loss: 0.4158 - val_acc: 0.8538 - val_categorical_crossentropy: 0.4158\n",
            "Epoch 207/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4125 - acc: 0.8569 - categorical_crossentropy: 0.4125 - val_loss: 0.4297 - val_acc: 0.8560 - val_categorical_crossentropy: 0.4297\n",
            "Epoch 208/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4155 - acc: 0.8558 - categorical_crossentropy: 0.4155 - val_loss: 0.4383 - val_acc: 0.8517 - val_categorical_crossentropy: 0.4383\n",
            "Epoch 209/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4163 - acc: 0.8558 - categorical_crossentropy: 0.4163 - val_loss: 0.3968 - val_acc: 0.8629 - val_categorical_crossentropy: 0.3968\n",
            "Epoch 210/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4137 - acc: 0.8563 - categorical_crossentropy: 0.4137 - val_loss: 0.4554 - val_acc: 0.8379 - val_categorical_crossentropy: 0.4554\n",
            "Epoch 211/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4136 - acc: 0.8567 - categorical_crossentropy: 0.4136 - val_loss: 0.4480 - val_acc: 0.8408 - val_categorical_crossentropy: 0.4480\n",
            "Epoch 212/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4121 - acc: 0.8569 - categorical_crossentropy: 0.4121 - val_loss: 0.4509 - val_acc: 0.8368 - val_categorical_crossentropy: 0.4509\n",
            "Epoch 213/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4129 - acc: 0.8573 - categorical_crossentropy: 0.4129 - val_loss: 0.4120 - val_acc: 0.8527 - val_categorical_crossentropy: 0.4120\n",
            "Epoch 214/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4128 - acc: 0.8562 - categorical_crossentropy: 0.4128 - val_loss: 0.4797 - val_acc: 0.8360 - val_categorical_crossentropy: 0.4797\n",
            "Epoch 215/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4125 - acc: 0.8566 - categorical_crossentropy: 0.4125 - val_loss: 0.4003 - val_acc: 0.8588 - val_categorical_crossentropy: 0.4003\n",
            "Epoch 216/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4117 - acc: 0.8571 - categorical_crossentropy: 0.4117 - val_loss: 0.7477 - val_acc: 0.7455 - val_categorical_crossentropy: 0.7477\n",
            "Epoch 217/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4130 - acc: 0.8569 - categorical_crossentropy: 0.4130 - val_loss: 0.4269 - val_acc: 0.8514 - val_categorical_crossentropy: 0.4269\n",
            "Epoch 218/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4129 - acc: 0.8570 - categorical_crossentropy: 0.4129 - val_loss: 0.4578 - val_acc: 0.8385 - val_categorical_crossentropy: 0.4578\n",
            "Epoch 219/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4135 - acc: 0.8564 - categorical_crossentropy: 0.4135 - val_loss: 0.4040 - val_acc: 0.8595 - val_categorical_crossentropy: 0.4040\n",
            "Epoch 220/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4141 - acc: 0.8562 - categorical_crossentropy: 0.4141 - val_loss: 0.4068 - val_acc: 0.8565 - val_categorical_crossentropy: 0.4068\n",
            "Epoch 221/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4110 - acc: 0.8563 - categorical_crossentropy: 0.4110 - val_loss: 0.4359 - val_acc: 0.8444 - val_categorical_crossentropy: 0.4359\n",
            "Epoch 222/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4114 - acc: 0.8581 - categorical_crossentropy: 0.4114 - val_loss: 0.3922 - val_acc: 0.8633 - val_categorical_crossentropy: 0.3922\n",
            "Epoch 223/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4118 - acc: 0.8575 - categorical_crossentropy: 0.4118 - val_loss: 0.4228 - val_acc: 0.8563 - val_categorical_crossentropy: 0.4228\n",
            "Epoch 224/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4114 - acc: 0.8573 - categorical_crossentropy: 0.4114 - val_loss: 0.4615 - val_acc: 0.8327 - val_categorical_crossentropy: 0.4615\n",
            "Epoch 225/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4125 - acc: 0.8564 - categorical_crossentropy: 0.4125 - val_loss: 0.4181 - val_acc: 0.8598 - val_categorical_crossentropy: 0.4181\n",
            "Epoch 226/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4110 - acc: 0.8569 - categorical_crossentropy: 0.4110 - val_loss: 0.4313 - val_acc: 0.8464 - val_categorical_crossentropy: 0.4313\n",
            "Epoch 227/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4141 - acc: 0.8559 - categorical_crossentropy: 0.4141 - val_loss: 0.3990 - val_acc: 0.8616 - val_categorical_crossentropy: 0.3990\n",
            "Epoch 228/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4127 - acc: 0.8565 - categorical_crossentropy: 0.4127 - val_loss: 0.3849 - val_acc: 0.8668 - val_categorical_crossentropy: 0.3849\n",
            "Epoch 229/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4111 - acc: 0.8567 - categorical_crossentropy: 0.4111 - val_loss: 0.4340 - val_acc: 0.8483 - val_categorical_crossentropy: 0.4340\n",
            "Epoch 230/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4114 - acc: 0.8570 - categorical_crossentropy: 0.4114 - val_loss: 0.4076 - val_acc: 0.8585 - val_categorical_crossentropy: 0.4076\n",
            "Epoch 231/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4113 - acc: 0.8568 - categorical_crossentropy: 0.4113 - val_loss: 0.3970 - val_acc: 0.8632 - val_categorical_crossentropy: 0.3970\n",
            "Epoch 232/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4112 - acc: 0.8568 - categorical_crossentropy: 0.4112 - val_loss: 0.3806 - val_acc: 0.8675 - val_categorical_crossentropy: 0.3806\n",
            "Epoch 233/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4098 - acc: 0.8581 - categorical_crossentropy: 0.4098 - val_loss: 0.4488 - val_acc: 0.8406 - val_categorical_crossentropy: 0.4488\n",
            "Epoch 234/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4089 - acc: 0.8583 - categorical_crossentropy: 0.4089 - val_loss: 0.4784 - val_acc: 0.8240 - val_categorical_crossentropy: 0.4784\n",
            "Epoch 235/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4111 - acc: 0.8569 - categorical_crossentropy: 0.4111 - val_loss: 0.4283 - val_acc: 0.8503 - val_categorical_crossentropy: 0.4283\n",
            "Epoch 236/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4112 - acc: 0.8570 - categorical_crossentropy: 0.4112 - val_loss: 0.4919 - val_acc: 0.8203 - val_categorical_crossentropy: 0.4919\n",
            "Epoch 237/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4107 - acc: 0.8573 - categorical_crossentropy: 0.4107 - val_loss: 0.3912 - val_acc: 0.8639 - val_categorical_crossentropy: 0.3912\n",
            "Epoch 238/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4109 - acc: 0.8569 - categorical_crossentropy: 0.4109 - val_loss: 0.4579 - val_acc: 0.8361 - val_categorical_crossentropy: 0.4579\n",
            "Epoch 239/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4096 - acc: 0.8579 - categorical_crossentropy: 0.4096 - val_loss: 0.4743 - val_acc: 0.8340 - val_categorical_crossentropy: 0.4743\n",
            "Epoch 240/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4093 - acc: 0.8583 - categorical_crossentropy: 0.4093 - val_loss: 0.5315 - val_acc: 0.8059 - val_categorical_crossentropy: 0.5315\n",
            "Epoch 241/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4098 - acc: 0.8569 - categorical_crossentropy: 0.4098 - val_loss: 0.4065 - val_acc: 0.8577 - val_categorical_crossentropy: 0.4065\n",
            "Epoch 242/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4091 - acc: 0.8577 - categorical_crossentropy: 0.4091 - val_loss: 0.4080 - val_acc: 0.8561 - val_categorical_crossentropy: 0.4080\n",
            "Epoch 243/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4097 - acc: 0.8567 - categorical_crossentropy: 0.4097 - val_loss: 0.3951 - val_acc: 0.8635 - val_categorical_crossentropy: 0.3951\n",
            "Epoch 244/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4080 - acc: 0.8579 - categorical_crossentropy: 0.4080 - val_loss: 0.3920 - val_acc: 0.8648 - val_categorical_crossentropy: 0.3920\n",
            "Epoch 245/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4091 - acc: 0.8583 - categorical_crossentropy: 0.4091 - val_loss: 0.4301 - val_acc: 0.8551 - val_categorical_crossentropy: 0.4301\n",
            "Epoch 246/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4089 - acc: 0.8579 - categorical_crossentropy: 0.4089 - val_loss: 0.5036 - val_acc: 0.8164 - val_categorical_crossentropy: 0.5036\n",
            "Epoch 247/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4084 - acc: 0.8577 - categorical_crossentropy: 0.4084 - val_loss: 0.4167 - val_acc: 0.8527 - val_categorical_crossentropy: 0.4167\n",
            "Epoch 248/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4094 - acc: 0.8580 - categorical_crossentropy: 0.4094 - val_loss: 0.5573 - val_acc: 0.7996 - val_categorical_crossentropy: 0.5573\n",
            "Epoch 249/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4085 - acc: 0.8585 - categorical_crossentropy: 0.4085 - val_loss: 0.4434 - val_acc: 0.8471 - val_categorical_crossentropy: 0.4434\n",
            "Epoch 250/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4087 - acc: 0.8587 - categorical_crossentropy: 0.4087 - val_loss: 0.3999 - val_acc: 0.8583 - val_categorical_crossentropy: 0.3999\n",
            "Epoch 251/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4080 - acc: 0.8587 - categorical_crossentropy: 0.4080 - val_loss: 0.4031 - val_acc: 0.8577 - val_categorical_crossentropy: 0.4031\n",
            "Epoch 252/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4074 - acc: 0.8583 - categorical_crossentropy: 0.4074 - val_loss: 0.6796 - val_acc: 0.7651 - val_categorical_crossentropy: 0.6796\n",
            "Epoch 253/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4085 - acc: 0.8581 - categorical_crossentropy: 0.4085 - val_loss: 0.3913 - val_acc: 0.8656 - val_categorical_crossentropy: 0.3913\n",
            "Epoch 254/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4059 - acc: 0.8587 - categorical_crossentropy: 0.4059 - val_loss: 0.4276 - val_acc: 0.8512 - val_categorical_crossentropy: 0.4276\n",
            "Epoch 255/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4081 - acc: 0.8585 - categorical_crossentropy: 0.4081 - val_loss: 0.3935 - val_acc: 0.8618 - val_categorical_crossentropy: 0.3935\n",
            "Epoch 256/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4062 - acc: 0.8584 - categorical_crossentropy: 0.4062 - val_loss: 0.4552 - val_acc: 0.8361 - val_categorical_crossentropy: 0.4552\n",
            "Epoch 257/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4068 - acc: 0.8583 - categorical_crossentropy: 0.4068 - val_loss: 0.4373 - val_acc: 0.8490 - val_categorical_crossentropy: 0.4373\n",
            "Epoch 258/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4058 - acc: 0.8592 - categorical_crossentropy: 0.4058 - val_loss: 0.4048 - val_acc: 0.8605 - val_categorical_crossentropy: 0.4048\n",
            "Epoch 259/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4076 - acc: 0.8578 - categorical_crossentropy: 0.4076 - val_loss: 0.4160 - val_acc: 0.8527 - val_categorical_crossentropy: 0.4160\n",
            "Epoch 260/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4054 - acc: 0.8580 - categorical_crossentropy: 0.4054 - val_loss: 0.4715 - val_acc: 0.8273 - val_categorical_crossentropy: 0.4715\n",
            "Epoch 261/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4069 - acc: 0.8587 - categorical_crossentropy: 0.4069 - val_loss: 0.3978 - val_acc: 0.8605 - val_categorical_crossentropy: 0.3978\n",
            "Epoch 262/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4068 - acc: 0.8583 - categorical_crossentropy: 0.4068 - val_loss: 0.4645 - val_acc: 0.8352 - val_categorical_crossentropy: 0.4645\n",
            "Epoch 263/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4074 - acc: 0.8580 - categorical_crossentropy: 0.4074 - val_loss: 0.4675 - val_acc: 0.8324 - val_categorical_crossentropy: 0.4675\n",
            "Epoch 264/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4066 - acc: 0.8573 - categorical_crossentropy: 0.4066 - val_loss: 0.4214 - val_acc: 0.8559 - val_categorical_crossentropy: 0.4214\n",
            "Epoch 265/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4052 - acc: 0.8584 - categorical_crossentropy: 0.4052 - val_loss: 0.4027 - val_acc: 0.8563 - val_categorical_crossentropy: 0.4027\n",
            "Epoch 266/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4061 - acc: 0.8590 - categorical_crossentropy: 0.4061 - val_loss: 0.4477 - val_acc: 0.8410 - val_categorical_crossentropy: 0.4477\n",
            "Epoch 267/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4054 - acc: 0.8582 - categorical_crossentropy: 0.4054 - val_loss: 0.4137 - val_acc: 0.8535 - val_categorical_crossentropy: 0.4137\n",
            "Epoch 268/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4068 - acc: 0.8588 - categorical_crossentropy: 0.4068 - val_loss: 0.4045 - val_acc: 0.8585 - val_categorical_crossentropy: 0.4045\n",
            "Epoch 269/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4050 - acc: 0.8588 - categorical_crossentropy: 0.4050 - val_loss: 0.4310 - val_acc: 0.8512 - val_categorical_crossentropy: 0.4310\n",
            "Epoch 270/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4058 - acc: 0.8590 - categorical_crossentropy: 0.4058 - val_loss: 0.3939 - val_acc: 0.8631 - val_categorical_crossentropy: 0.3939\n",
            "Epoch 271/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4053 - acc: 0.8586 - categorical_crossentropy: 0.4053 - val_loss: 0.4062 - val_acc: 0.8583 - val_categorical_crossentropy: 0.4062\n",
            "Epoch 272/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4052 - acc: 0.8586 - categorical_crossentropy: 0.4052 - val_loss: 0.4039 - val_acc: 0.8587 - val_categorical_crossentropy: 0.4039\n",
            "Epoch 273/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4049 - acc: 0.8584 - categorical_crossentropy: 0.4049 - val_loss: 0.4153 - val_acc: 0.8549 - val_categorical_crossentropy: 0.4153\n",
            "Epoch 274/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4035 - acc: 0.8593 - categorical_crossentropy: 0.4035 - val_loss: 0.4348 - val_acc: 0.8471 - val_categorical_crossentropy: 0.4348\n",
            "Epoch 275/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4048 - acc: 0.8595 - categorical_crossentropy: 0.4048 - val_loss: 0.4174 - val_acc: 0.8570 - val_categorical_crossentropy: 0.4174\n",
            "Epoch 276/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4031 - acc: 0.8593 - categorical_crossentropy: 0.4031 - val_loss: 0.4529 - val_acc: 0.8418 - val_categorical_crossentropy: 0.4529\n",
            "Epoch 277/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4041 - acc: 0.8589 - categorical_crossentropy: 0.4041 - val_loss: 0.4664 - val_acc: 0.8339 - val_categorical_crossentropy: 0.4664\n",
            "Epoch 278/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4051 - acc: 0.8592 - categorical_crossentropy: 0.4051 - val_loss: 0.4204 - val_acc: 0.8589 - val_categorical_crossentropy: 0.4204\n",
            "Epoch 279/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4040 - acc: 0.8594 - categorical_crossentropy: 0.4040 - val_loss: 0.4243 - val_acc: 0.8509 - val_categorical_crossentropy: 0.4243\n",
            "Epoch 280/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4029 - acc: 0.8589 - categorical_crossentropy: 0.4029 - val_loss: 0.4009 - val_acc: 0.8618 - val_categorical_crossentropy: 0.4009\n",
            "Epoch 281/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4034 - acc: 0.8592 - categorical_crossentropy: 0.4034 - val_loss: 0.4059 - val_acc: 0.8594 - val_categorical_crossentropy: 0.4059\n",
            "Epoch 282/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4028 - acc: 0.8594 - categorical_crossentropy: 0.4028 - val_loss: 0.4667 - val_acc: 0.8330 - val_categorical_crossentropy: 0.4667\n",
            "Epoch 283/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4059 - acc: 0.8585 - categorical_crossentropy: 0.4059 - val_loss: 0.4146 - val_acc: 0.8534 - val_categorical_crossentropy: 0.4146\n",
            "Epoch 284/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4013 - acc: 0.8607 - categorical_crossentropy: 0.4013 - val_loss: 0.4139 - val_acc: 0.8541 - val_categorical_crossentropy: 0.4139\n",
            "Epoch 285/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4032 - acc: 0.8595 - categorical_crossentropy: 0.4032 - val_loss: 0.5236 - val_acc: 0.8228 - val_categorical_crossentropy: 0.5236\n",
            "Epoch 286/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4040 - acc: 0.8594 - categorical_crossentropy: 0.4040 - val_loss: 0.5021 - val_acc: 0.8178 - val_categorical_crossentropy: 0.5021\n",
            "Epoch 287/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4038 - acc: 0.8593 - categorical_crossentropy: 0.4038 - val_loss: 0.4795 - val_acc: 0.8243 - val_categorical_crossentropy: 0.4795\n",
            "Epoch 288/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.4024 - acc: 0.8602 - categorical_crossentropy: 0.4024 - val_loss: 0.4340 - val_acc: 0.8556 - val_categorical_crossentropy: 0.4340\n",
            "Epoch 289/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4010 - acc: 0.8601 - categorical_crossentropy: 0.4010 - val_loss: 0.4131 - val_acc: 0.8563 - val_categorical_crossentropy: 0.4131\n",
            "Epoch 290/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4022 - acc: 0.8597 - categorical_crossentropy: 0.4022 - val_loss: 0.4586 - val_acc: 0.8486 - val_categorical_crossentropy: 0.4586\n",
            "Epoch 291/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4037 - acc: 0.8593 - categorical_crossentropy: 0.4037 - val_loss: 0.4477 - val_acc: 0.8397 - val_categorical_crossentropy: 0.4477\n",
            "Epoch 292/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4024 - acc: 0.8596 - categorical_crossentropy: 0.4024 - val_loss: 0.4070 - val_acc: 0.8579 - val_categorical_crossentropy: 0.4070\n",
            "Epoch 293/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4022 - acc: 0.8597 - categorical_crossentropy: 0.4022 - val_loss: 0.4340 - val_acc: 0.8466 - val_categorical_crossentropy: 0.4340\n",
            "Epoch 294/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4022 - acc: 0.8593 - categorical_crossentropy: 0.4022 - val_loss: 0.3876 - val_acc: 0.8662 - val_categorical_crossentropy: 0.3876\n",
            "Epoch 295/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4021 - acc: 0.8600 - categorical_crossentropy: 0.4021 - val_loss: 0.3975 - val_acc: 0.8629 - val_categorical_crossentropy: 0.3975\n",
            "Epoch 296/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4021 - acc: 0.8593 - categorical_crossentropy: 0.4021 - val_loss: 0.4089 - val_acc: 0.8575 - val_categorical_crossentropy: 0.4089\n",
            "Epoch 297/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4018 - acc: 0.8601 - categorical_crossentropy: 0.4018 - val_loss: 0.4045 - val_acc: 0.8580 - val_categorical_crossentropy: 0.4045\n",
            "Epoch 298/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4016 - acc: 0.8592 - categorical_crossentropy: 0.4016 - val_loss: 0.4175 - val_acc: 0.8568 - val_categorical_crossentropy: 0.4175\n",
            "Epoch 299/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4020 - acc: 0.8590 - categorical_crossentropy: 0.4020 - val_loss: 0.4153 - val_acc: 0.8551 - val_categorical_crossentropy: 0.4153\n",
            "Epoch 300/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4012 - acc: 0.8600 - categorical_crossentropy: 0.4012 - val_loss: 0.4952 - val_acc: 0.8200 - val_categorical_crossentropy: 0.4952\n",
            "Epoch 301/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4015 - acc: 0.8601 - categorical_crossentropy: 0.4015 - val_loss: 0.3829 - val_acc: 0.8658 - val_categorical_crossentropy: 0.3829\n",
            "Epoch 302/2000\n",
            "139839/139839 [==============================] - 7s 48us/step - loss: 0.4012 - acc: 0.8602 - categorical_crossentropy: 0.4012 - val_loss: 0.4056 - val_acc: 0.8607 - val_categorical_crossentropy: 0.4056\n",
            "Epoch 303/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4019 - acc: 0.8594 - categorical_crossentropy: 0.4019 - val_loss: 0.4655 - val_acc: 0.8339 - val_categorical_crossentropy: 0.4655\n",
            "Epoch 304/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4028 - acc: 0.8595 - categorical_crossentropy: 0.4028 - val_loss: 0.3954 - val_acc: 0.8603 - val_categorical_crossentropy: 0.3954\n",
            "Epoch 305/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4013 - acc: 0.8605 - categorical_crossentropy: 0.4013 - val_loss: 0.3910 - val_acc: 0.8649 - val_categorical_crossentropy: 0.3910\n",
            "Epoch 306/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4016 - acc: 0.8598 - categorical_crossentropy: 0.4016 - val_loss: 0.4356 - val_acc: 0.8461 - val_categorical_crossentropy: 0.4356\n",
            "Epoch 307/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4012 - acc: 0.8593 - categorical_crossentropy: 0.4012 - val_loss: 0.4061 - val_acc: 0.8563 - val_categorical_crossentropy: 0.4061\n",
            "Epoch 308/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.3996 - acc: 0.8602 - categorical_crossentropy: 0.3996 - val_loss: 0.4441 - val_acc: 0.8560 - val_categorical_crossentropy: 0.4441\n",
            "Epoch 309/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4019 - acc: 0.8596 - categorical_crossentropy: 0.4019 - val_loss: 0.4580 - val_acc: 0.8410 - val_categorical_crossentropy: 0.4580\n",
            "Epoch 310/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.4021 - acc: 0.8602 - categorical_crossentropy: 0.4021 - val_loss: 0.4264 - val_acc: 0.8512 - val_categorical_crossentropy: 0.4264\n",
            "Epoch 311/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4006 - acc: 0.8606 - categorical_crossentropy: 0.4006 - val_loss: 0.4339 - val_acc: 0.8524 - val_categorical_crossentropy: 0.4339\n",
            "Epoch 312/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4001 - acc: 0.8605 - categorical_crossentropy: 0.4001 - val_loss: 0.3949 - val_acc: 0.8635 - val_categorical_crossentropy: 0.3949\n",
            "Epoch 313/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.3984 - acc: 0.8608 - categorical_crossentropy: 0.3984 - val_loss: 0.4522 - val_acc: 0.8387 - val_categorical_crossentropy: 0.4522\n",
            "Epoch 314/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.3994 - acc: 0.8604 - categorical_crossentropy: 0.3994 - val_loss: 0.4062 - val_acc: 0.8569 - val_categorical_crossentropy: 0.4062\n",
            "Epoch 315/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4000 - acc: 0.8598 - categorical_crossentropy: 0.4000 - val_loss: 0.4042 - val_acc: 0.8570 - val_categorical_crossentropy: 0.4042\n",
            "Epoch 316/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.3996 - acc: 0.8603 - categorical_crossentropy: 0.3996 - val_loss: 0.3890 - val_acc: 0.8627 - val_categorical_crossentropy: 0.3890\n",
            "Epoch 317/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.3997 - acc: 0.8601 - categorical_crossentropy: 0.3997 - val_loss: 0.3932 - val_acc: 0.8646 - val_categorical_crossentropy: 0.3932\n",
            "Epoch 318/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4004 - acc: 0.8603 - categorical_crossentropy: 0.4004 - val_loss: 0.4571 - val_acc: 0.8351 - val_categorical_crossentropy: 0.4571\n",
            "Epoch 319/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.3983 - acc: 0.8612 - categorical_crossentropy: 0.3983 - val_loss: 0.7257 - val_acc: 0.7353 - val_categorical_crossentropy: 0.7257\n",
            "Epoch 320/2000\n",
            "139839/139839 [==============================] - 7s 47us/step - loss: 0.3997 - acc: 0.8607 - categorical_crossentropy: 0.3997 - val_loss: 0.3982 - val_acc: 0.8640 - val_categorical_crossentropy: 0.3982\n",
            "Epoch 321/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.3989 - acc: 0.8606 - categorical_crossentropy: 0.3989 - val_loss: 0.4108 - val_acc: 0.8580 - val_categorical_crossentropy: 0.4108\n",
            "Epoch 322/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.3987 - acc: 0.8608 - categorical_crossentropy: 0.3987 - val_loss: 0.4338 - val_acc: 0.8441 - val_categorical_crossentropy: 0.4338\n",
            "Epoch 323/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.3998 - acc: 0.8604 - categorical_crossentropy: 0.3998 - val_loss: 0.4037 - val_acc: 0.8596 - val_categorical_crossentropy: 0.4037\n",
            "Epoch 324/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4001 - acc: 0.8595 - categorical_crossentropy: 0.4001 - val_loss: 0.4037 - val_acc: 0.8606 - val_categorical_crossentropy: 0.4037\n",
            "Epoch 325/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.4012 - acc: 0.8599 - categorical_crossentropy: 0.4012 - val_loss: 0.4306 - val_acc: 0.8481 - val_categorical_crossentropy: 0.4306\n",
            "Epoch 326/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.3986 - acc: 0.8609 - categorical_crossentropy: 0.3986 - val_loss: 0.6035 - val_acc: 0.7817 - val_categorical_crossentropy: 0.6035\n",
            "Epoch 327/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.3982 - acc: 0.8606 - categorical_crossentropy: 0.3982 - val_loss: 0.4719 - val_acc: 0.8333 - val_categorical_crossentropy: 0.4719\n",
            "Epoch 328/2000\n",
            "139839/139839 [==============================] - 6s 44us/step - loss: 0.3981 - acc: 0.8612 - categorical_crossentropy: 0.3981 - val_loss: 0.4170 - val_acc: 0.8542 - val_categorical_crossentropy: 0.4170\n",
            "Epoch 329/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.3981 - acc: 0.8607 - categorical_crossentropy: 0.3981 - val_loss: 0.4635 - val_acc: 0.8350 - val_categorical_crossentropy: 0.4635\n",
            "Epoch 330/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.3980 - acc: 0.8610 - categorical_crossentropy: 0.3980 - val_loss: 0.4009 - val_acc: 0.8599 - val_categorical_crossentropy: 0.4009\n",
            "Epoch 331/2000\n",
            "139839/139839 [==============================] - 6s 46us/step - loss: 0.4006 - acc: 0.8596 - categorical_crossentropy: 0.4006 - val_loss: 0.4222 - val_acc: 0.8508 - val_categorical_crossentropy: 0.4222\n",
            "Epoch 332/2000\n",
            "139839/139839 [==============================] - 6s 45us/step - loss: 0.3974 - acc: 0.8602 - categorical_crossentropy: 0.3974 - val_loss: 0.4584 - val_acc: 0.8376 - val_categorical_crossentropy: 0.4584\n",
            "Epoch 00332: early stopping\n",
            "59931/59931 [==============================] - 3s 54us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.45836401136193866, 0.8376132552564592, 0.45836401136193866]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04uzx-RuIdI-",
        "colab_type": "code",
        "outputId": "0e37717c-eec2-4d29-9f93-457c8102fe9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "### 최적 모델 불러오기\n",
        "best_model = keras.models.load_model('best_model.h5')\n",
        "val_loss, val_acc, val_category = best_model.evaluate(X_test, y_test)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
            "59931/59931 [==============================] - 4s 68us/sample - loss: 0.3806 - acc: 0.8675 - categorical_crossentropy: 0.3806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbtg6S6AIdJE",
        "colab_type": "code",
        "outputId": "8e06021c-85c7-4c22-e009-662723deda56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "fig, loss_ax = plt.subplots()\n",
        "# acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "loss_ax.legend(loc='upper left')\n",
        "\n",
        "# acc_ax.plot(history.history['categorical_crossentropy'], 'b', label='train crossentropy')\n",
        "# acc_ax.plot(history.history['val_categorical_crossentropy'], 'g', label='val crossentropy')\n",
        "# acc_ax.set_ylabel('categorical_crossentropy')\n",
        "# acc_ax.legend(loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noCiKEueoECiKosQ5KgSKoihxzv8D75El8IUrWbsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1lPPJBiIdJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_y_pred = best_model.predict(test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwLvngasIdJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.DataFrame(data=real_y_pred, columns=sample_submission_df.columns, index=sample_submission_df.index)\n",
        "submission.to_csv('submission_keras_renewal_0.3749   .csv', index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUfxKxAUIdJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 기록 1.레이어 2. 옴티마이저  ==> val_loss\n",
        "#1. 9개,// 2. 0.0003 ==> 0.404\n",
        "#1. 8개(256*2)층 하나 제거 //2. 0.0001 ==> 0.666179819053121\n",
        "#1. 8개 //2. 0.0005 ==> 0.47\n",
        "#1. 7개 //2. 0.0002 ==> 0.38\n",
        "#1. 8개(256*1)층 하나 추가 //2. 0.0002 ==> 0.0.3935\n",
        "#1. 8개(파생변수만 남기고 전부 제거) // 2. 0.0002 ==>0.5719 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA-2NZ7XuSJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}